{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.optimizers import RMSprop\n",
    "import rbflayer, kmeans_initializer\n",
    "from rbflayer import InitCentersRandom\n",
    "from keras.initializers import RandomUniform, Initializer, Constant\n",
    "\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, Activation, Dropout\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from scipy import * \n",
    "from scipy.linalg import norm, pinv\n",
    "from matplotlib import pyplot as plt  \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq # Used to read the data\n",
    "import os \n",
    "import numpy as np\n",
    "from keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm # Processing time measurement\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class\n",
    "from keras import optimizers # Allow us to access the Adam class to modify some parameters\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model\n",
    "from keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.keras.layers import CuDNNGRU\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras import initializers,regularizers,constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICE\"]  = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_A = pd.read_excel(io=r'C:\\Users\\Serendipity\\Attention_RBF\\Feature_A.xlsx')\n",
    "Feature_B = pd.read_excel(io=r'C:\\Users\\Serendipity\\Attention_RBF\\Feature_B.xlsx')\n",
    "Feature_C = pd.read_excel(io=r'C:\\Users\\Serendipity\\Attention_RBF\\Feature_c.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_A = Feature_A.rename(columns={\"Isa_A\": \"Isa\", \"Isd_A\": \"Isd\",\"Iar_A\": \"Iar\",\"Pvd_A\": \"Pvd\",\"θ_A\":\"θ\"})\n",
    "Feature_B = Feature_B.rename(columns={\"Isa_B\": \"Isa\", \"Isd_B\": \"Isd\",\"Iar_B\": \"Iar\",\"Pvd_B\": \"Pvd\",\"θ_B\":\"θ\"})\n",
    "Feature_C = Feature_C.rename(columns={\"Isa_C\": \"Isa\", \"Isd_C\": \"Isd\",\"Iar_C\": \"Iar\",\"Pvd_C\": \"Pvd\",\"θ_C\":\"θ\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature = pd.concat([\n",
    "    Feature_A,\n",
    "    Feature_B,\n",
    "    Feature_C,\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_y = Feature['label']\n",
    "X_feat =  Feature[['负载率','Isa','三相电流不平衡度','Isd','Iar','Pvd','负载率差值','θ']]\n",
    "# float64->float32\n",
    "#X_feat[X_feat.select_dtypes(np.float64).columns] = X_feat.select_dtypes(np.float64).astype(np.float32)\n",
    "\n",
    "# inf,nan数据填充\n",
    "X_feat = (X_feat.replace([np.inf, -np.inf], np.nan)).fillna(value = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE样本均衡化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 149618, 1: 1834})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# 查看所生成的样本类别分布，0和1样本比例9比1，属于类别不平衡数据\n",
    "print(Counter(label_y))\n",
    "# Counter({0: 900, 1: 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用imlbearn库中上采样方法中的SMOTE接口\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# 定义SMOTE模型，random_state相当于随机数种子的作用\n",
    "smo = SMOTE(random_state=42)\n",
    "X_smo, y_smo = smo.fit_resample(X_feat, label_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 149618, 1: 149618})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(y_smo))\n",
    "# Counter({0: 900, 1: 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler((0,1))\n",
    "\n",
    "lb = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>负载率</th>\n",
       "      <th>Isa</th>\n",
       "      <th>三相电流不平衡度</th>\n",
       "      <th>Isd</th>\n",
       "      <th>Iar</th>\n",
       "      <th>Pvd</th>\n",
       "      <th>负载率差值</th>\n",
       "      <th>θ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.373800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>-1.659500</td>\n",
       "      <td>0.006229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.714300</td>\n",
       "      <td>-7.931262</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>-6.172840</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>1.227800</td>\n",
       "      <td>-0.078344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.942100</td>\n",
       "      <td>-14.276272</td>\n",
       "      <td>0.316327</td>\n",
       "      <td>-10.465116</td>\n",
       "      <td>0.136508</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>1.853900</td>\n",
       "      <td>-0.140728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.796000</td>\n",
       "      <td>38.070059</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>25.263158</td>\n",
       "      <td>0.150794</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>-3.316500</td>\n",
       "      <td>0.353894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.479500</td>\n",
       "      <td>3.172505</td>\n",
       "      <td>0.231214</td>\n",
       "      <td>2.816901</td>\n",
       "      <td>0.112698</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.261900</td>\n",
       "      <td>0.034775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299231</th>\n",
       "      <td>16.318183</td>\n",
       "      <td>-22.278998</td>\n",
       "      <td>0.251495</td>\n",
       "      <td>-20.864044</td>\n",
       "      <td>0.146751</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>1.918612</td>\n",
       "      <td>-0.218812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299232</th>\n",
       "      <td>5.064354</td>\n",
       "      <td>-84.854526</td>\n",
       "      <td>0.279909</td>\n",
       "      <td>-151.887609</td>\n",
       "      <td>0.036940</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>2.766760</td>\n",
       "      <td>-0.700622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299233</th>\n",
       "      <td>19.471195</td>\n",
       "      <td>-23.951392</td>\n",
       "      <td>0.386530</td>\n",
       "      <td>-17.749873</td>\n",
       "      <td>0.091627</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>1.796548</td>\n",
       "      <td>-0.233068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299234</th>\n",
       "      <td>18.119446</td>\n",
       "      <td>-50.202186</td>\n",
       "      <td>0.568227</td>\n",
       "      <td>-132.136228</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>14.167156</td>\n",
       "      <td>-0.457158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299235</th>\n",
       "      <td>24.059451</td>\n",
       "      <td>11.087138</td>\n",
       "      <td>0.079039</td>\n",
       "      <td>10.255191</td>\n",
       "      <td>0.172603</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>-1.565689</td>\n",
       "      <td>0.108278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299236 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              负载率        Isa  三相电流不平衡度         Isd       Iar       Pvd  \\\n",
       "0       14.373800   0.000000  0.221106    0.000000  0.128571  0.000222   \n",
       "1       12.714300  -7.931262  0.372881   -6.172840  0.128571  0.000222   \n",
       "2       13.942100 -14.276272  0.316327  -10.465116  0.136508  0.000270   \n",
       "3       15.796000  38.070059  0.295455   25.263158  0.150794  0.000413   \n",
       "4       12.479500   3.172505  0.231214    2.816901  0.112698  0.000032   \n",
       "...           ...        ...       ...         ...       ...       ...   \n",
       "299231  16.318183 -22.278998  0.251495  -20.864044  0.146751  0.000391   \n",
       "299232   5.064354 -84.854526  0.279909 -151.887609  0.036940  0.000578   \n",
       "299233  19.471195 -23.951392  0.386530  -17.749873  0.091627  0.000229   \n",
       "299234  18.119446 -50.202186  0.568227 -132.136228  0.076000  0.001348   \n",
       "299235  24.059451  11.087138  0.079039   10.255191  0.172603  0.000179   \n",
       "\n",
       "            负载率差值         θ  \n",
       "0       -1.659500  0.006229  \n",
       "1        1.227800 -0.078344  \n",
       "2        1.853900 -0.140728  \n",
       "3       -3.316500  0.353894  \n",
       "4       -0.261900  0.034775  \n",
       "...           ...       ...  \n",
       "299231   1.918612 -0.218812  \n",
       "299232   2.766760 -0.700622  \n",
       "299233   1.796548 -0.233068  \n",
       "299234  14.167156 -0.457158  \n",
       "299235  -1.565689  0.108278  \n",
       "\n",
       "[299236 rows x 8 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_smo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n",
    "\n",
    "class Attention(keras.layers.Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape = (input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is NN LSTM Model creation\n",
    "def model_lstm(input_shape):\n",
    "    # The shape was explained above, must have this order\n",
    "    inp = Input(shape=(input_shape[1], input_shape[2],))\n",
    "    # This is the LSTM layer\n",
    "    # Bidirecional implies that the 160 chunks are calculated in both ways, 0 to 159 and 159 to zero\n",
    "    # although it appear that just 0 to 159 way matter, I have tested with and without, and tha later worked best\n",
    "    # 128 and 64 are the number of cells used, too many can overfit and too few can underfit\n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(inp)\n",
    "    # The second LSTM can give more fire power to the model, but can overfit it too\n",
    "    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "    # Attention is a new tecnology that can be applyed to a Recurrent NN to give more meanings to a signal found in the middle\n",
    "    # of the data, it helps more in longs chains of data. A normal RNN give all the responsibility of detect the signal\n",
    "    # to the last cell. Google RNN Attention for more information :)\n",
    "    x = Attention(input_shape[1])(x)\n",
    "    # A intermediate full connected (Dense) can help to deal with nonlinears outputs\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    # A binnary classification as this must finish with shape (1,)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    # Pay attention in the addition of matthews_correlation metric in the compilation, it is a success factor key\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[matthews_correlation])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select how many folds will be created\n",
    "N_SPLITS = 5\n",
    "# it is just a constant with the measurements data size\n",
    "X = np.reshape(X_smo.values,(-1,8,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is the official metric used in this competition\n",
    "# below is the declaration of a function used inside the keras model, calculation with K (keras backend / thensorflow)\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    '''Calculates the Matthews correlation coefficient measure for quality\n",
    "    of binary classification problems.\n",
    "    '''\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning fold 1\n",
      "Train on 239388 samples, validate on 59848 samples\n",
      "Epoch 1/100\n",
      "239388/239388 [==============================] - ETA: 0s - loss: 0.6178 - matthews_correlation: 0.3334\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.33848, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 12s 51us/sample - loss: 0.6178 - matthews_correlation: 0.3334 - val_loss: 0.6128 - val_matthews_correlation: 0.3385\n",
      "Epoch 2/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.6019 - matthews_correlation: 0.3524\n",
      "Epoch 00002: val_matthews_correlation improved from 0.33848 to 0.35508, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 27us/sample - loss: 0.6019 - matthews_correlation: 0.3522 - val_loss: 0.5948 - val_matthews_correlation: 0.3551\n",
      "Epoch 3/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.5872 - matthews_correlation: 0.3650\n",
      "Epoch 00003: val_matthews_correlation improved from 0.35508 to 0.36755, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 6s 27us/sample - loss: 0.5871 - matthews_correlation: 0.3651 - val_loss: 0.5884 - val_matthews_correlation: 0.3676\n",
      "Epoch 4/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.5766 - matthews_correlation: 0.3786\n",
      "Epoch 00004: val_matthews_correlation improved from 0.36755 to 0.38405, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 6s 27us/sample - loss: 0.5766 - matthews_correlation: 0.3786 - val_loss: 0.5735 - val_matthews_correlation: 0.3840\n",
      "Epoch 5/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.5649 - matthews_correlation: 0.3965\n",
      "Epoch 00005: val_matthews_correlation improved from 0.38405 to 0.41684, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 27us/sample - loss: 0.5649 - matthews_correlation: 0.3965 - val_loss: 0.5603 - val_matthews_correlation: 0.4168\n",
      "Epoch 6/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.5506 - matthews_correlation: 0.4184\n",
      "Epoch 00006: val_matthews_correlation improved from 0.41684 to 0.43881, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.5505 - matthews_correlation: 0.4185 - val_loss: 0.5412 - val_matthews_correlation: 0.4388\n",
      "Epoch 7/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.5316 - matthews_correlation: 0.4486\n",
      "Epoch 00007: val_matthews_correlation improved from 0.43881 to 0.45844, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.5314 - matthews_correlation: 0.4490 - val_loss: 0.5257 - val_matthews_correlation: 0.4584\n",
      "Epoch 8/100\n",
      "239388/239388 [==============================] - ETA: 0s - loss: 0.5106 - matthews_correlation: 0.4852\n",
      "Epoch 00008: val_matthews_correlation improved from 0.45844 to 0.49610, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.5106 - matthews_correlation: 0.4852 - val_loss: 0.5072 - val_matthews_correlation: 0.4961\n",
      "Epoch 9/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.4860 - matthews_correlation: 0.5208\n",
      "Epoch 00009: val_matthews_correlation improved from 0.49610 to 0.52766, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.4859 - matthews_correlation: 0.5210 - val_loss: 0.4791 - val_matthews_correlation: 0.5277\n",
      "Epoch 10/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.4633 - matthews_correlation: 0.5503\n",
      "Epoch 00010: val_matthews_correlation improved from 0.52766 to 0.55687, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.4633 - matthews_correlation: 0.5503 - val_loss: 0.4589 - val_matthews_correlation: 0.5569\n",
      "Epoch 11/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.4405 - matthews_correlation: 0.5808\n",
      "Epoch 00011: val_matthews_correlation improved from 0.55687 to 0.58998, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.4405 - matthews_correlation: 0.5809 - val_loss: 0.4334 - val_matthews_correlation: 0.5900\n",
      "Epoch 12/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.4208 - matthews_correlation: 0.6070\n",
      "Epoch 00012: val_matthews_correlation improved from 0.58998 to 0.61517, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.4207 - matthews_correlation: 0.6072 - val_loss: 0.4197 - val_matthews_correlation: 0.6152\n",
      "Epoch 13/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.3967 - matthews_correlation: 0.6356\n",
      "Epoch 00013: val_matthews_correlation improved from 0.61517 to 0.64687, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.3966 - matthews_correlation: 0.6359 - val_loss: 0.3939 - val_matthews_correlation: 0.6469\n",
      "Epoch 14/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.3796 - matthews_correlation: 0.6563\n",
      "Epoch 00014: val_matthews_correlation improved from 0.64687 to 0.66385, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.3795 - matthews_correlation: 0.6563 - val_loss: 0.3764 - val_matthews_correlation: 0.6639\n",
      "Epoch 15/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.3614 - matthews_correlation: 0.6795\n",
      "Epoch 00015: val_matthews_correlation improved from 0.66385 to 0.67715, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 27us/sample - loss: 0.3613 - matthews_correlation: 0.6797 - val_loss: 0.3665 - val_matthews_correlation: 0.6772\n",
      "Epoch 16/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.3468 - matthews_correlation: 0.6958\n",
      "Epoch 00016: val_matthews_correlation improved from 0.67715 to 0.68874, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.3469 - matthews_correlation: 0.6958 - val_loss: 0.3538 - val_matthews_correlation: 0.6887\n",
      "Epoch 17/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.3286 - matthews_correlation: 0.7181\n",
      "Epoch 00017: val_matthews_correlation improved from 0.68874 to 0.71155, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.3286 - matthews_correlation: 0.7181 - val_loss: 0.3345 - val_matthews_correlation: 0.7116\n",
      "Epoch 18/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.3189 - matthews_correlation: 0.7276\n",
      "Epoch 00018: val_matthews_correlation improved from 0.71155 to 0.72328, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.3189 - matthews_correlation: 0.7277 - val_loss: 0.3299 - val_matthews_correlation: 0.7233\n",
      "Epoch 19/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.3064 - matthews_correlation: 0.7416\n",
      "Epoch 00019: val_matthews_correlation improved from 0.72328 to 0.72983, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.3064 - matthews_correlation: 0.7417 - val_loss: 0.3235 - val_matthews_correlation: 0.7298\n",
      "Epoch 20/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.2948 - matthews_correlation: 0.7535\n",
      "Epoch 00020: val_matthews_correlation improved from 0.72983 to 0.73908, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.2946 - matthews_correlation: 0.7538 - val_loss: 0.3109 - val_matthews_correlation: 0.7391\n",
      "Epoch 21/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.2836 - matthews_correlation: 0.7664\n",
      "Epoch 00021: val_matthews_correlation improved from 0.73908 to 0.75382, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.2837 - matthews_correlation: 0.7663 - val_loss: 0.2977 - val_matthews_correlation: 0.7538\n",
      "Epoch 22/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.2749 - matthews_correlation: 0.7745\n",
      "Epoch 00022: val_matthews_correlation improved from 0.75382 to 0.75990, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.2749 - matthews_correlation: 0.7744 - val_loss: 0.2884 - val_matthews_correlation: 0.7599\n",
      "Epoch 23/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.2631 - matthews_correlation: 0.7862\n",
      "Epoch 00023: val_matthews_correlation improved from 0.75990 to 0.77022, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.2632 - matthews_correlation: 0.7862 - val_loss: 0.2809 - val_matthews_correlation: 0.7702\n",
      "Epoch 24/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.2557 - matthews_correlation: 0.7933\n",
      "Epoch 00024: val_matthews_correlation improved from 0.77022 to 0.77425, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.2557 - matthews_correlation: 0.7933 - val_loss: 0.2795 - val_matthews_correlation: 0.7743\n",
      "Epoch 25/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.2524 - matthews_correlation: 0.7970\n",
      "Epoch 00025: val_matthews_correlation improved from 0.77425 to 0.78503, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.2523 - matthews_correlation: 0.7972 - val_loss: 0.2671 - val_matthews_correlation: 0.7850\n",
      "Epoch 26/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.2448 - matthews_correlation: 0.8047\n",
      "Epoch 00026: val_matthews_correlation did not improve from 0.78503\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.2447 - matthews_correlation: 0.8047 - val_loss: 0.2723 - val_matthews_correlation: 0.7787\n",
      "Epoch 27/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.2388 - matthews_correlation: 0.8090\n",
      "Epoch 00027: val_matthews_correlation improved from 0.78503 to 0.79752, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.2388 - matthews_correlation: 0.8090 - val_loss: 0.2535 - val_matthews_correlation: 0.7975\n",
      "Epoch 28/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.2320 - matthews_correlation: 0.8149\n",
      "Epoch 00028: val_matthews_correlation improved from 0.79752 to 0.80082, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.2320 - matthews_correlation: 0.8149 - val_loss: 0.2529 - val_matthews_correlation: 0.8008\n",
      "Epoch 29/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.2281 - matthews_correlation: 0.8193\n",
      "Epoch 00029: val_matthews_correlation improved from 0.80082 to 0.80652, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.2280 - matthews_correlation: 0.8194 - val_loss: 0.2456 - val_matthews_correlation: 0.8065\n",
      "Epoch 30/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.2211 - matthews_correlation: 0.8250\n",
      "Epoch 00030: val_matthews_correlation improved from 0.80652 to 0.80946, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.2211 - matthews_correlation: 0.8249 - val_loss: 0.2413 - val_matthews_correlation: 0.8095\n",
      "Epoch 31/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.2144 - matthews_correlation: 0.8325\n",
      "Epoch 00031: val_matthews_correlation improved from 0.80946 to 0.81550, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.2144 - matthews_correlation: 0.8325 - val_loss: 0.2360 - val_matthews_correlation: 0.8155\n",
      "Epoch 32/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.2093 - matthews_correlation: 0.8360\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.81550\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.2092 - matthews_correlation: 0.8360 - val_loss: 0.2363 - val_matthews_correlation: 0.8139\n",
      "Epoch 33/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.2088 - matthews_correlation: 0.8373\n",
      "Epoch 00033: val_matthews_correlation improved from 0.81550 to 0.81923, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.2088 - matthews_correlation: 0.8373 - val_loss: 0.2342 - val_matthews_correlation: 0.8192\n",
      "Epoch 34/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.2015 - matthews_correlation: 0.8431\n",
      "Epoch 00034: val_matthews_correlation improved from 0.81923 to 0.82814, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.2015 - matthews_correlation: 0.8432 - val_loss: 0.2248 - val_matthews_correlation: 0.8281\n",
      "Epoch 35/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1928 - matthews_correlation: 0.8504\n",
      "Epoch 00035: val_matthews_correlation did not improve from 0.82814\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1928 - matthews_correlation: 0.8504 - val_loss: 0.2241 - val_matthews_correlation: 0.8248\n",
      "Epoch 36/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.1955 - matthews_correlation: 0.8482\n",
      "Epoch 00036: val_matthews_correlation improved from 0.82814 to 0.83569, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1955 - matthews_correlation: 0.8483 - val_loss: 0.2177 - val_matthews_correlation: 0.8357\n",
      "Epoch 37/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1863 - matthews_correlation: 0.8564\n",
      "Epoch 00037: val_matthews_correlation improved from 0.83569 to 0.83783, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1863 - matthews_correlation: 0.8565 - val_loss: 0.2131 - val_matthews_correlation: 0.8378\n",
      "Epoch 38/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1809 - matthews_correlation: 0.8608\n",
      "Epoch 00038: val_matthews_correlation improved from 0.83783 to 0.84119, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1809 - matthews_correlation: 0.8608 - val_loss: 0.2094 - val_matthews_correlation: 0.8412\n",
      "Epoch 39/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1787 - matthews_correlation: 0.8620\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.84119\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1788 - matthews_correlation: 0.8620 - val_loss: 0.2096 - val_matthews_correlation: 0.8412\n",
      "Epoch 40/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1777 - matthews_correlation: 0.8640\n",
      "Epoch 00040: val_matthews_correlation improved from 0.84119 to 0.84776, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1776 - matthews_correlation: 0.8641 - val_loss: 0.2043 - val_matthews_correlation: 0.8478\n",
      "Epoch 41/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1694 - matthews_correlation: 0.8709\n",
      "Epoch 00041: val_matthews_correlation did not improve from 0.84776\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1694 - matthews_correlation: 0.8709 - val_loss: 0.2074 - val_matthews_correlation: 0.8474\n",
      "Epoch 42/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1707 - matthews_correlation: 0.8700\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.84776\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1707 - matthews_correlation: 0.8701 - val_loss: 0.2064 - val_matthews_correlation: 0.8449\n",
      "Epoch 43/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1662 - matthews_correlation: 0.8739\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.84776\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1663 - matthews_correlation: 0.8737 - val_loss: 0.2054 - val_matthews_correlation: 0.8471\n",
      "Epoch 44/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1669 - matthews_correlation: 0.8725\n",
      "Epoch 00044: val_matthews_correlation improved from 0.84776 to 0.85029, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1669 - matthews_correlation: 0.8724 - val_loss: 0.1954 - val_matthews_correlation: 0.8503\n",
      "Epoch 45/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1607 - matthews_correlation: 0.8771\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.85029\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1607 - matthews_correlation: 0.8771 - val_loss: 0.2057 - val_matthews_correlation: 0.8420\n",
      "Epoch 46/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.1553 - matthews_correlation: 0.8824\n",
      "Epoch 00046: val_matthews_correlation improved from 0.85029 to 0.85468, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1554 - matthews_correlation: 0.8823 - val_loss: 0.1971 - val_matthews_correlation: 0.8547\n",
      "Epoch 47/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1550 - matthews_correlation: 0.8833\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.85468\n",
      "239388/239388 [==============================] - 7s 27us/sample - loss: 0.1549 - matthews_correlation: 0.8834 - val_loss: 0.2048 - val_matthews_correlation: 0.8532\n",
      "Epoch 48/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.1514 - matthews_correlation: 0.8851\n",
      "Epoch 00048: val_matthews_correlation improved from 0.85468 to 0.86911, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1513 - matthews_correlation: 0.8853 - val_loss: 0.1783 - val_matthews_correlation: 0.8691\n",
      "Epoch 49/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1487 - matthews_correlation: 0.8874\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.86911\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1487 - matthews_correlation: 0.8874 - val_loss: 0.2027 - val_matthews_correlation: 0.8497\n",
      "Epoch 50/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1456 - matthews_correlation: 0.8910\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.86911\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1456 - matthews_correlation: 0.8910 - val_loss: 0.1981 - val_matthews_correlation: 0.8501\n",
      "Epoch 51/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1433 - matthews_correlation: 0.8931\n",
      "Epoch 00051: val_matthews_correlation did not improve from 0.86911\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1432 - matthews_correlation: 0.8931 - val_loss: 0.1846 - val_matthews_correlation: 0.8662\n",
      "Epoch 52/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1435 - matthews_correlation: 0.8925\n",
      "Epoch 00052: val_matthews_correlation did not improve from 0.86911\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1435 - matthews_correlation: 0.8924 - val_loss: 0.1888 - val_matthews_correlation: 0.8624\n",
      "Epoch 53/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1384 - matthews_correlation: 0.8966\n",
      "Epoch 00053: val_matthews_correlation improved from 0.86911 to 0.87003, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1384 - matthews_correlation: 0.8966 - val_loss: 0.1831 - val_matthews_correlation: 0.8700\n",
      "Epoch 54/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1390 - matthews_correlation: 0.8968\n",
      "Epoch 00054: val_matthews_correlation improved from 0.87003 to 0.87011, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1391 - matthews_correlation: 0.8967 - val_loss: 0.1799 - val_matthews_correlation: 0.8701\n",
      "Epoch 55/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1343 - matthews_correlation: 0.9008\n",
      "Epoch 00055: val_matthews_correlation improved from 0.87011 to 0.87037, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1343 - matthews_correlation: 0.9008 - val_loss: 0.1800 - val_matthews_correlation: 0.8704\n",
      "Epoch 56/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1341 - matthews_correlation: 0.9004\n",
      "Epoch 00056: val_matthews_correlation improved from 0.87037 to 0.87481, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1342 - matthews_correlation: 0.9003 - val_loss: 0.1755 - val_matthews_correlation: 0.8748\n",
      "Epoch 57/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.1280 - matthews_correlation: 0.9055\n",
      "Epoch 00057: val_matthews_correlation improved from 0.87481 to 0.87500, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1281 - matthews_correlation: 0.9054 - val_loss: 0.1715 - val_matthews_correlation: 0.8750\n",
      "Epoch 58/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.1330 - matthews_correlation: 0.9010\n",
      "Epoch 00058: val_matthews_correlation did not improve from 0.87500\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1329 - matthews_correlation: 0.9011 - val_loss: 0.1831 - val_matthews_correlation: 0.8673\n",
      "Epoch 59/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.1258 - matthews_correlation: 0.9076\n",
      "Epoch 00059: val_matthews_correlation improved from 0.87500 to 0.88402, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1258 - matthews_correlation: 0.9075 - val_loss: 0.1621 - val_matthews_correlation: 0.8840\n",
      "Epoch 60/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1260 - matthews_correlation: 0.9072\n",
      "Epoch 00060: val_matthews_correlation did not improve from 0.88402\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1259 - matthews_correlation: 0.9073 - val_loss: 0.1710 - val_matthews_correlation: 0.8787\n",
      "Epoch 61/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1207 - matthews_correlation: 0.9108\n",
      "Epoch 00061: val_matthews_correlation did not improve from 0.88402\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1206 - matthews_correlation: 0.9109 - val_loss: 0.1729 - val_matthews_correlation: 0.8800\n",
      "Epoch 62/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1191 - matthews_correlation: 0.9131\n",
      "Epoch 00062: val_matthews_correlation did not improve from 0.88402\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1192 - matthews_correlation: 0.9130 - val_loss: 0.1688 - val_matthews_correlation: 0.8837\n",
      "Epoch 63/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1171 - matthews_correlation: 0.9140\n",
      "Epoch 00063: val_matthews_correlation did not improve from 0.88402\n",
      "239388/239388 [==============================] - 7s 29us/sample - loss: 0.1172 - matthews_correlation: 0.9140 - val_loss: 0.1753 - val_matthews_correlation: 0.8722\n",
      "Epoch 64/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1227 - matthews_correlation: 0.9102\n",
      "Epoch 00064: val_matthews_correlation did not improve from 0.88402\n",
      "239388/239388 [==============================] - 7s 29us/sample - loss: 0.1228 - matthews_correlation: 0.9101 - val_loss: 0.1682 - val_matthews_correlation: 0.8814\n",
      "Epoch 65/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1135 - matthews_correlation: 0.9161\n",
      "Epoch 00065: val_matthews_correlation improved from 0.88402 to 0.89028, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1135 - matthews_correlation: 0.9161 - val_loss: 0.1606 - val_matthews_correlation: 0.8903\n",
      "Epoch 66/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1140 - matthews_correlation: 0.9159\n",
      "Epoch 00066: val_matthews_correlation did not improve from 0.89028\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1140 - matthews_correlation: 0.9158 - val_loss: 0.1708 - val_matthews_correlation: 0.8817\n",
      "Epoch 67/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1178 - matthews_correlation: 0.9135\n",
      "Epoch 00067: val_matthews_correlation did not improve from 0.89028\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1177 - matthews_correlation: 0.9136 - val_loss: 0.1624 - val_matthews_correlation: 0.8881\n",
      "Epoch 68/100\n",
      "239388/239388 [==============================] - ETA: 0s - loss: 0.1093 - matthews_correlation: 0.9198\n",
      "Epoch 00068: val_matthews_correlation did not improve from 0.89028\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1093 - matthews_correlation: 0.9198 - val_loss: 0.1596 - val_matthews_correlation: 0.8882\n",
      "Epoch 69/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.1095 - matthews_correlation: 0.9213\n",
      "Epoch 00069: val_matthews_correlation did not improve from 0.89028\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1096 - matthews_correlation: 0.9213 - val_loss: 0.1903 - val_matthews_correlation: 0.8724\n",
      "Epoch 70/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1082 - matthews_correlation: 0.9218\n",
      "Epoch 00070: val_matthews_correlation improved from 0.89028 to 0.89333, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1082 - matthews_correlation: 0.9218 - val_loss: 0.1573 - val_matthews_correlation: 0.8933\n",
      "Epoch 71/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1075 - matthews_correlation: 0.9201\n",
      "Epoch 00071: val_matthews_correlation did not improve from 0.89333\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1075 - matthews_correlation: 0.9201 - val_loss: 0.1622 - val_matthews_correlation: 0.8912\n",
      "Epoch 72/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1041 - matthews_correlation: 0.9243\n",
      "Epoch 00072: val_matthews_correlation improved from 0.89333 to 0.89482, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1041 - matthews_correlation: 0.9243 - val_loss: 0.1578 - val_matthews_correlation: 0.8948\n",
      "Epoch 73/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1013 - matthews_correlation: 0.9268\n",
      "Epoch 00073: val_matthews_correlation did not improve from 0.89482\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1014 - matthews_correlation: 0.9267 - val_loss: 0.1662 - val_matthews_correlation: 0.8883\n",
      "Epoch 74/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1028 - matthews_correlation: 0.9252\n",
      "Epoch 00074: val_matthews_correlation did not improve from 0.89482\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1029 - matthews_correlation: 0.9251 - val_loss: 0.1601 - val_matthews_correlation: 0.8937\n",
      "Epoch 75/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1009 - matthews_correlation: 0.9265\n",
      "Epoch 00075: val_matthews_correlation improved from 0.89482 to 0.90168, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1009 - matthews_correlation: 0.9264 - val_loss: 0.1484 - val_matthews_correlation: 0.9017\n",
      "Epoch 76/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.1050 - matthews_correlation: 0.9235\n",
      "Epoch 00076: val_matthews_correlation did not improve from 0.90168\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.1049 - matthews_correlation: 0.9236 - val_loss: 0.1689 - val_matthews_correlation: 0.8864\n",
      "Epoch 77/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0996 - matthews_correlation: 0.9275\n",
      "Epoch 00077: val_matthews_correlation did not improve from 0.90168\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0996 - matthews_correlation: 0.9274 - val_loss: 0.1480 - val_matthews_correlation: 0.9011\n",
      "Epoch 78/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0965 - matthews_correlation: 0.9306\n",
      "Epoch 00078: val_matthews_correlation did not improve from 0.90168\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0964 - matthews_correlation: 0.9307 - val_loss: 0.1504 - val_matthews_correlation: 0.9017\n",
      "Epoch 79/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0925 - matthews_correlation: 0.9336\n",
      "Epoch 00079: val_matthews_correlation improved from 0.90168 to 0.90346, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0924 - matthews_correlation: 0.9337 - val_loss: 0.1478 - val_matthews_correlation: 0.9035\n",
      "Epoch 80/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.0973 - matthews_correlation: 0.9303\n",
      "Epoch 00080: val_matthews_correlation improved from 0.90346 to 0.90356, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0973 - matthews_correlation: 0.9302 - val_loss: 0.1449 - val_matthews_correlation: 0.9036\n",
      "Epoch 81/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0955 - matthews_correlation: 0.9314\n",
      "Epoch 00081: val_matthews_correlation did not improve from 0.90356\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0955 - matthews_correlation: 0.9314 - val_loss: 0.1484 - val_matthews_correlation: 0.9018\n",
      "Epoch 82/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0928 - matthews_correlation: 0.9337\n",
      "Epoch 00082: val_matthews_correlation improved from 0.90356 to 0.90613, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0928 - matthews_correlation: 0.9337 - val_loss: 0.1461 - val_matthews_correlation: 0.9061\n",
      "Epoch 83/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0918 - matthews_correlation: 0.9345\n",
      "Epoch 00083: val_matthews_correlation did not improve from 0.90613\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0918 - matthews_correlation: 0.9346 - val_loss: 0.1599 - val_matthews_correlation: 0.8925\n",
      "Epoch 84/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0909 - matthews_correlation: 0.9352\n",
      "Epoch 00084: val_matthews_correlation did not improve from 0.90613\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0909 - matthews_correlation: 0.9353 - val_loss: 0.1571 - val_matthews_correlation: 0.9015\n",
      "Epoch 85/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0863 - matthews_correlation: 0.9385\n",
      "Epoch 00085: val_matthews_correlation improved from 0.90613 to 0.91248, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0863 - matthews_correlation: 0.9385 - val_loss: 0.1347 - val_matthews_correlation: 0.9125\n",
      "Epoch 86/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0918 - matthews_correlation: 0.9348\n",
      "Epoch 00086: val_matthews_correlation did not improve from 0.91248\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0917 - matthews_correlation: 0.9348 - val_loss: 0.1463 - val_matthews_correlation: 0.9036\n",
      "Epoch 87/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0871 - matthews_correlation: 0.9377\n",
      "Epoch 00087: val_matthews_correlation did not improve from 0.91248\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0871 - matthews_correlation: 0.9377 - val_loss: 0.1534 - val_matthews_correlation: 0.9002\n",
      "Epoch 88/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0894 - matthews_correlation: 0.9356\n",
      "Epoch 00088: val_matthews_correlation did not improve from 0.91248\n",
      "239388/239388 [==============================] - 7s 29us/sample - loss: 0.0894 - matthews_correlation: 0.9356 - val_loss: 0.1466 - val_matthews_correlation: 0.9046\n",
      "Epoch 89/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.0827 - matthews_correlation: 0.9407\n",
      "Epoch 00089: val_matthews_correlation did not improve from 0.91248\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0828 - matthews_correlation: 0.9406 - val_loss: 0.1552 - val_matthews_correlation: 0.9014\n",
      "Epoch 90/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.0840 - matthews_correlation: 0.9397\n",
      "Epoch 00090: val_matthews_correlation did not improve from 0.91248\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0840 - matthews_correlation: 0.9397 - val_loss: 0.1429 - val_matthews_correlation: 0.9102\n",
      "Epoch 91/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.0850 - matthews_correlation: 0.9398\n",
      "Epoch 00091: val_matthews_correlation did not improve from 0.91248\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0851 - matthews_correlation: 0.9397 - val_loss: 0.1525 - val_matthews_correlation: 0.9016\n",
      "Epoch 92/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0834 - matthews_correlation: 0.9408\n",
      "Epoch 00092: val_matthews_correlation did not improve from 0.91248\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0834 - matthews_correlation: 0.9408 - val_loss: 0.1418 - val_matthews_correlation: 0.9092\n",
      "Epoch 93/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0797 - matthews_correlation: 0.9431\n",
      "Epoch 00093: val_matthews_correlation did not improve from 0.91248\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0797 - matthews_correlation: 0.9431 - val_loss: 0.1459 - val_matthews_correlation: 0.9073\n",
      "Epoch 94/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0827 - matthews_correlation: 0.9410\n",
      "Epoch 00094: val_matthews_correlation did not improve from 0.91248\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0828 - matthews_correlation: 0.9409 - val_loss: 0.1515 - val_matthews_correlation: 0.8992\n",
      "Epoch 95/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0817 - matthews_correlation: 0.9416\n",
      "Epoch 00095: val_matthews_correlation improved from 0.91248 to 0.91327, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0816 - matthews_correlation: 0.9417 - val_loss: 0.1419 - val_matthews_correlation: 0.9133\n",
      "Epoch 96/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0798 - matthews_correlation: 0.9426\n",
      "Epoch 00096: val_matthews_correlation improved from 0.91327 to 0.91670, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0798 - matthews_correlation: 0.9425 - val_loss: 0.1333 - val_matthews_correlation: 0.9167\n",
      "Epoch 97/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0730 - matthews_correlation: 0.9482\n",
      "Epoch 00097: val_matthews_correlation did not improve from 0.91670\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0730 - matthews_correlation: 0.9482 - val_loss: 0.1389 - val_matthews_correlation: 0.9113\n",
      "Epoch 98/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0820 - matthews_correlation: 0.9419\n",
      "Epoch 00098: val_matthews_correlation did not improve from 0.91670\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0820 - matthews_correlation: 0.9419 - val_loss: 0.1528 - val_matthews_correlation: 0.9044\n",
      "Epoch 99/100\n",
      "238592/239388 [============================>.] - ETA: 0s - loss: 0.0745 - matthews_correlation: 0.9479\n",
      "Epoch 00099: val_matthews_correlation improved from 0.91670 to 0.91799, saving model to weights_0.h5\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0745 - matthews_correlation: 0.9478 - val_loss: 0.1328 - val_matthews_correlation: 0.9180\n",
      "Epoch 100/100\n",
      "237568/239388 [============================>.] - ETA: 0s - loss: 0.0748 - matthews_correlation: 0.9463\n",
      "Epoch 00100: val_matthews_correlation did not improve from 0.91799\n",
      "239388/239388 [==============================] - 7s 28us/sample - loss: 0.0749 - matthews_correlation: 0.9463 - val_loss: 0.1398 - val_matthews_correlation: 0.9135\n",
      "Beginning fold 2\n",
      "Train on 239389 samples, validate on 59847 samples\n",
      "Epoch 1/100\n",
      "239389/239389 [==============================] - ETA: 0s - loss: 0.6181 - matthews_correlation: 0.3298 ETA: 0s - loss: 0.6182 - matthews_correlation: 0.\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.34520, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 13s 56us/sample - loss: 0.6181 - matthews_correlation: 0.3298 - val_loss: 0.6114 - val_matthews_correlation: 0.3452\n",
      "Epoch 2/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.6037 - matthews_correlation: 0.3521\n",
      "Epoch 00002: val_matthews_correlation improved from 0.34520 to 0.35950, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.6038 - matthews_correlation: 0.3519 - val_loss: 0.5946 - val_matthews_correlation: 0.3595\n",
      "Epoch 3/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.5900 - matthews_correlation: 0.3635\n",
      "Epoch 00003: val_matthews_correlation improved from 0.35950 to 0.36474, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5899 - matthews_correlation: 0.3636 - val_loss: 0.5899 - val_matthews_correlation: 0.3647\n",
      "Epoch 4/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.5772 - matthews_correlation: 0.3796\n",
      "Epoch 00004: val_matthews_correlation improved from 0.36474 to 0.38490, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5772 - matthews_correlation: 0.3798 - val_loss: 0.5727 - val_matthews_correlation: 0.3849\n",
      "Epoch 5/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.5620 - matthews_correlation: 0.4003\n",
      "Epoch 00005: val_matthews_correlation improved from 0.38490 to 0.41050, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5619 - matthews_correlation: 0.4005 - val_loss: 0.5553 - val_matthews_correlation: 0.4105\n",
      "Epoch 6/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.5444 - matthews_correlation: 0.4286\n",
      "Epoch 00006: val_matthews_correlation improved from 0.41050 to 0.45165, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5444 - matthews_correlation: 0.4289 - val_loss: 0.5319 - val_matthews_correlation: 0.4516\n",
      "Epoch 7/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.5216 - matthews_correlation: 0.4639\n",
      "Epoch 00007: val_matthews_correlation improved from 0.45165 to 0.47707, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5215 - matthews_correlation: 0.4643 - val_loss: 0.5139 - val_matthews_correlation: 0.4771\n",
      "Epoch 8/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.4972 - matthews_correlation: 0.4986\n",
      "Epoch 00008: val_matthews_correlation improved from 0.47707 to 0.50600, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.4971 - matthews_correlation: 0.4989 - val_loss: 0.4888 - val_matthews_correlation: 0.5060\n",
      "Epoch 9/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.4725 - matthews_correlation: 0.5327\n",
      "Epoch 00009: val_matthews_correlation improved from 0.50600 to 0.54082, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.4725 - matthews_correlation: 0.5327 - val_loss: 0.4701 - val_matthews_correlation: 0.5408\n",
      "Epoch 10/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.4525 - matthews_correlation: 0.5643\n",
      "Epoch 00010: val_matthews_correlation improved from 0.54082 to 0.55302, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.4524 - matthews_correlation: 0.5644 - val_loss: 0.4601 - val_matthews_correlation: 0.5530\n",
      "Epoch 11/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.4329 - matthews_correlation: 0.5912\n",
      "Epoch 00011: val_matthews_correlation improved from 0.55302 to 0.59592, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.4328 - matthews_correlation: 0.5913 - val_loss: 0.4264 - val_matthews_correlation: 0.5959\n",
      "Epoch 12/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.4105 - matthews_correlation: 0.6181\n",
      "Epoch 00012: val_matthews_correlation improved from 0.59592 to 0.63047, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.4106 - matthews_correlation: 0.6180 - val_loss: 0.4026 - val_matthews_correlation: 0.6305\n",
      "Epoch 13/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3920 - matthews_correlation: 0.6420\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.63047\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3920 - matthews_correlation: 0.6419 - val_loss: 0.4024 - val_matthews_correlation: 0.6201\n",
      "Epoch 14/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3748 - matthews_correlation: 0.6659\n",
      "Epoch 00014: val_matthews_correlation improved from 0.63047 to 0.66049, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3747 - matthews_correlation: 0.6660 - val_loss: 0.3763 - val_matthews_correlation: 0.6605\n",
      "Epoch 15/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3595 - matthews_correlation: 0.6821\n",
      "Epoch 00015: val_matthews_correlation improved from 0.66049 to 0.68171, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3594 - matthews_correlation: 0.6824 - val_loss: 0.3674 - val_matthews_correlation: 0.6817\n",
      "Epoch 16/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3442 - matthews_correlation: 0.7011\n",
      "Epoch 00016: val_matthews_correlation improved from 0.68171 to 0.70175, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3442 - matthews_correlation: 0.7011 - val_loss: 0.3427 - val_matthews_correlation: 0.7018\n",
      "Epoch 17/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3326 - matthews_correlation: 0.7136\n",
      "Epoch 00017: val_matthews_correlation improved from 0.70175 to 0.70220, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3326 - matthews_correlation: 0.7137 - val_loss: 0.3491 - val_matthews_correlation: 0.7022\n",
      "Epoch 18/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3170 - matthews_correlation: 0.7315\n",
      "Epoch 00018: val_matthews_correlation improved from 0.70220 to 0.71710, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3169 - matthews_correlation: 0.7315 - val_loss: 0.3281 - val_matthews_correlation: 0.7171\n",
      "Epoch 19/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3080 - matthews_correlation: 0.7414\n",
      "Epoch 00019: val_matthews_correlation improved from 0.71710 to 0.74175, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3080 - matthews_correlation: 0.7416 - val_loss: 0.3112 - val_matthews_correlation: 0.7418\n",
      "Epoch 20/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2958 - matthews_correlation: 0.7542\n",
      "Epoch 00020: val_matthews_correlation did not improve from 0.74175\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2958 - matthews_correlation: 0.7542 - val_loss: 0.3104 - val_matthews_correlation: 0.7365\n",
      "Epoch 21/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.2877 - matthews_correlation: 0.7627\n",
      "Epoch 00021: val_matthews_correlation improved from 0.74175 to 0.74762, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2876 - matthews_correlation: 0.7627 - val_loss: 0.3011 - val_matthews_correlation: 0.7476\n",
      "Epoch 22/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2796 - matthews_correlation: 0.7708\n",
      "Epoch 00022: val_matthews_correlation improved from 0.74762 to 0.75295, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2796 - matthews_correlation: 0.7708 - val_loss: 0.2924 - val_matthews_correlation: 0.7529\n",
      "Epoch 23/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2704 - matthews_correlation: 0.7804\n",
      "Epoch 00023: val_matthews_correlation improved from 0.75295 to 0.76664, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2703 - matthews_correlation: 0.7804 - val_loss: 0.2824 - val_matthews_correlation: 0.7666\n",
      "Epoch 24/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2605 - matthews_correlation: 0.7899\n",
      "Epoch 00024: val_matthews_correlation improved from 0.76664 to 0.78118, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2605 - matthews_correlation: 0.7899 - val_loss: 0.2694 - val_matthews_correlation: 0.7812\n",
      "Epoch 25/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2562 - matthews_correlation: 0.7935\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.78118\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2562 - matthews_correlation: 0.7935 - val_loss: 0.2738 - val_matthews_correlation: 0.7782\n",
      "Epoch 26/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2479 - matthews_correlation: 0.8029\n",
      "Epoch 00026: val_matthews_correlation improved from 0.78118 to 0.78676, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2478 - matthews_correlation: 0.8030 - val_loss: 0.2623 - val_matthews_correlation: 0.7868\n",
      "Epoch 27/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2400 - matthews_correlation: 0.8088\n",
      "Epoch 00027: val_matthews_correlation improved from 0.78676 to 0.78964, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2401 - matthews_correlation: 0.8088 - val_loss: 0.2633 - val_matthews_correlation: 0.7896\n",
      "Epoch 28/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2372 - matthews_correlation: 0.8126\n",
      "Epoch 00028: val_matthews_correlation improved from 0.78964 to 0.79578, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2371 - matthews_correlation: 0.8126 - val_loss: 0.2566 - val_matthews_correlation: 0.7958\n",
      "Epoch 29/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2277 - matthews_correlation: 0.8204\n",
      "Epoch 00029: val_matthews_correlation improved from 0.79578 to 0.81095, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2275 - matthews_correlation: 0.8205 - val_loss: 0.2426 - val_matthews_correlation: 0.8110\n",
      "Epoch 30/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2252 - matthews_correlation: 0.8228\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.81095\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2251 - matthews_correlation: 0.8229 - val_loss: 0.2624 - val_matthews_correlation: 0.7885\n",
      "Epoch 31/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.2203 - matthews_correlation: 0.8264\n",
      "Epoch 00031: val_matthews_correlation improved from 0.81095 to 0.81546, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2204 - matthews_correlation: 0.8263 - val_loss: 0.2385 - val_matthews_correlation: 0.8155\n",
      "Epoch 32/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.2125 - matthews_correlation: 0.8345\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.81546\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2126 - matthews_correlation: 0.8344 - val_loss: 0.2391 - val_matthews_correlation: 0.8085\n",
      "Epoch 33/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2058 - matthews_correlation: 0.8398\n",
      "Epoch 00033: val_matthews_correlation improved from 0.81546 to 0.81765, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2057 - matthews_correlation: 0.8398 - val_loss: 0.2345 - val_matthews_correlation: 0.8177\n",
      "Epoch 34/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2052 - matthews_correlation: 0.8413\n",
      "Epoch 00034: val_matthews_correlation improved from 0.81765 to 0.82369, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2053 - matthews_correlation: 0.8412 - val_loss: 0.2262 - val_matthews_correlation: 0.8237\n",
      "Epoch 35/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2001 - matthews_correlation: 0.8448\n",
      "Epoch 00035: val_matthews_correlation improved from 0.82369 to 0.82507, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2000 - matthews_correlation: 0.8449 - val_loss: 0.2227 - val_matthews_correlation: 0.8251\n",
      "Epoch 36/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1914 - matthews_correlation: 0.8534\n",
      "Epoch 00036: val_matthews_correlation improved from 0.82507 to 0.83046, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1914 - matthews_correlation: 0.8535 - val_loss: 0.2206 - val_matthews_correlation: 0.8305\n",
      "Epoch 37/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1954 - matthews_correlation: 0.8501\n",
      "Epoch 00037: val_matthews_correlation improved from 0.83046 to 0.83460, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1955 - matthews_correlation: 0.8501 - val_loss: 0.2157 - val_matthews_correlation: 0.8346\n",
      "Epoch 38/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1851 - matthews_correlation: 0.8579\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.83460\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1851 - matthews_correlation: 0.8579 - val_loss: 0.2158 - val_matthews_correlation: 0.8337\n",
      "Epoch 39/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1886 - matthews_correlation: 0.8554\n",
      "Epoch 00039: val_matthews_correlation did not improve from 0.83460\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1885 - matthews_correlation: 0.8555 - val_loss: 0.2343 - val_matthews_correlation: 0.8195\n",
      "Epoch 40/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1802 - matthews_correlation: 0.8621\n",
      "Epoch 00040: val_matthews_correlation improved from 0.83460 to 0.83515, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1803 - matthews_correlation: 0.8620 - val_loss: 0.2165 - val_matthews_correlation: 0.8351\n",
      "Epoch 41/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1734 - matthews_correlation: 0.8684\n",
      "Epoch 00041: val_matthews_correlation improved from 0.83515 to 0.84062, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1734 - matthews_correlation: 0.8683 - val_loss: 0.2077 - val_matthews_correlation: 0.8406\n",
      "Epoch 42/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1737 - matthews_correlation: 0.8685\n",
      "Epoch 00042: val_matthews_correlation improved from 0.84062 to 0.85045, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1737 - matthews_correlation: 0.8684 - val_loss: 0.1973 - val_matthews_correlation: 0.8504\n",
      "Epoch 43/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1710 - matthews_correlation: 0.8698\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.85045\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1711 - matthews_correlation: 0.8696 - val_loss: 0.2088 - val_matthews_correlation: 0.8422\n",
      "Epoch 44/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1655 - matthews_correlation: 0.8752\n",
      "Epoch 00044: val_matthews_correlation did not improve from 0.85045\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1655 - matthews_correlation: 0.8751 - val_loss: 0.2008 - val_matthews_correlation: 0.8504\n",
      "Epoch 45/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1640 - matthews_correlation: 0.8773\n",
      "Epoch 00045: val_matthews_correlation improved from 0.85045 to 0.85795, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1639 - matthews_correlation: 0.8774 - val_loss: 0.1899 - val_matthews_correlation: 0.8579\n",
      "Epoch 46/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1605 - matthews_correlation: 0.8791\n",
      "Epoch 00046: val_matthews_correlation improved from 0.85795 to 0.86043, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1605 - matthews_correlation: 0.8791 - val_loss: 0.1905 - val_matthews_correlation: 0.8604\n",
      "Epoch 47/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1592 - matthews_correlation: 0.8809\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.86043\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1593 - matthews_correlation: 0.8809 - val_loss: 0.2006 - val_matthews_correlation: 0.8508\n",
      "Epoch 48/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1552 - matthews_correlation: 0.8833\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.86043\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1552 - matthews_correlation: 0.8833 - val_loss: 0.2073 - val_matthews_correlation: 0.8491\n",
      "Epoch 49/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1533 - matthews_correlation: 0.8841\n",
      "Epoch 00049: val_matthews_correlation improved from 0.86043 to 0.86063, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1533 - matthews_correlation: 0.8841 - val_loss: 0.1861 - val_matthews_correlation: 0.8606\n",
      "Epoch 50/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1500 - matthews_correlation: 0.8880\n",
      "Epoch 00050: val_matthews_correlation improved from 0.86063 to 0.86471, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1500 - matthews_correlation: 0.8880 - val_loss: 0.1859 - val_matthews_correlation: 0.8647\n",
      "Epoch 51/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1498 - matthews_correlation: 0.8875\n",
      "Epoch 00051: val_matthews_correlation did not improve from 0.86471\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1498 - matthews_correlation: 0.8875 - val_loss: 0.1871 - val_matthews_correlation: 0.8616\n",
      "Epoch 52/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1452 - matthews_correlation: 0.8921\n",
      "Epoch 00052: val_matthews_correlation improved from 0.86471 to 0.86487, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1452 - matthews_correlation: 0.8921 - val_loss: 0.1884 - val_matthews_correlation: 0.8649\n",
      "Epoch 53/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1467 - matthews_correlation: 0.8901\n",
      "Epoch 00053: val_matthews_correlation did not improve from 0.86487\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1468 - matthews_correlation: 0.8899 - val_loss: 0.2098 - val_matthews_correlation: 0.8439\n",
      "Epoch 54/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1403 - matthews_correlation: 0.8949\n",
      "Epoch 00054: val_matthews_correlation improved from 0.86487 to 0.86909, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1404 - matthews_correlation: 0.8949 - val_loss: 0.1802 - val_matthews_correlation: 0.8691\n",
      "Epoch 55/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1373 - matthews_correlation: 0.8970\n",
      "Epoch 00055: val_matthews_correlation improved from 0.86909 to 0.87265, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1374 - matthews_correlation: 0.8969 - val_loss: 0.1786 - val_matthews_correlation: 0.8726\n",
      "Epoch 56/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1350 - matthews_correlation: 0.8998\n",
      "Epoch 00056: val_matthews_correlation did not improve from 0.87265\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1351 - matthews_correlation: 0.8997 - val_loss: 0.1877 - val_matthews_correlation: 0.8612\n",
      "Epoch 57/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1371 - matthews_correlation: 0.8985\n",
      "Epoch 00057: val_matthews_correlation did not improve from 0.87265\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1370 - matthews_correlation: 0.8987 - val_loss: 0.1886 - val_matthews_correlation: 0.8675\n",
      "Epoch 58/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1303 - matthews_correlation: 0.9034\n",
      "Epoch 00058: val_matthews_correlation improved from 0.87265 to 0.87715, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1304 - matthews_correlation: 0.9034 - val_loss: 0.1738 - val_matthews_correlation: 0.8771\n",
      "Epoch 59/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1288 - matthews_correlation: 0.9053\n",
      "Epoch 00059: val_matthews_correlation did not improve from 0.87715\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1288 - matthews_correlation: 0.9052 - val_loss: 0.1792 - val_matthews_correlation: 0.8737\n",
      "Epoch 60/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1271 - matthews_correlation: 0.9063 ETA: 0s - loss: 0.1\n",
      "Epoch 00060: val_matthews_correlation improved from 0.87715 to 0.87803, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1271 - matthews_correlation: 0.9063 - val_loss: 0.1746 - val_matthews_correlation: 0.8780\n",
      "Epoch 61/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1289 - matthews_correlation: 0.9052\n",
      "Epoch 00061: val_matthews_correlation did not improve from 0.87803\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1289 - matthews_correlation: 0.9051 - val_loss: 0.1910 - val_matthews_correlation: 0.8629\n",
      "Epoch 62/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1303 - matthews_correlation: 0.9029\n",
      "Epoch 00062: val_matthews_correlation did not improve from 0.87803\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1304 - matthews_correlation: 0.9027 - val_loss: 0.1715 - val_matthews_correlation: 0.8778\n",
      "Epoch 63/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1224 - matthews_correlation: 0.9097\n",
      "Epoch 00063: val_matthews_correlation improved from 0.87803 to 0.87893, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1225 - matthews_correlation: 0.9097 - val_loss: 0.1681 - val_matthews_correlation: 0.8789\n",
      "Epoch 64/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1218 - matthews_correlation: 0.9105\n",
      "Epoch 00064: val_matthews_correlation did not improve from 0.87893\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1219 - matthews_correlation: 0.9105 - val_loss: 0.1716 - val_matthews_correlation: 0.8748\n",
      "Epoch 65/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1177 - matthews_correlation: 0.9132\n",
      "Epoch 00065: val_matthews_correlation improved from 0.87893 to 0.88053, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1178 - matthews_correlation: 0.9131 - val_loss: 0.1705 - val_matthews_correlation: 0.8805\n",
      "Epoch 66/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1189 - matthews_correlation: 0.9131\n",
      "Epoch 00066: val_matthews_correlation improved from 0.88053 to 0.88432, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1188 - matthews_correlation: 0.9132 - val_loss: 0.1652 - val_matthews_correlation: 0.8843\n",
      "Epoch 67/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1156 - matthews_correlation: 0.9152\n",
      "Epoch 00067: val_matthews_correlation did not improve from 0.88432\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1156 - matthews_correlation: 0.9152 - val_loss: 0.1692 - val_matthews_correlation: 0.8800\n",
      "Epoch 68/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1142 - matthews_correlation: 0.9160\n",
      "Epoch 00068: val_matthews_correlation did not improve from 0.88432\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1142 - matthews_correlation: 0.9160 - val_loss: 0.1697 - val_matthews_correlation: 0.8828\n",
      "Epoch 69/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1132 - matthews_correlation: 0.9172 ETA: 0s - loss: 0\n",
      "Epoch 00069: val_matthews_correlation did not improve from 0.88432\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1132 - matthews_correlation: 0.9171 - val_loss: 0.1718 - val_matthews_correlation: 0.8806\n",
      "Epoch 70/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1121 - matthews_correlation: 0.9175\n",
      "Epoch 00070: val_matthews_correlation did not improve from 0.88432\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1123 - matthews_correlation: 0.9173 - val_loss: 0.1684 - val_matthews_correlation: 0.8820\n",
      "Epoch 71/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1116 - matthews_correlation: 0.9188\n",
      "Epoch 00071: val_matthews_correlation improved from 0.88432 to 0.88495, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1115 - matthews_correlation: 0.9188 - val_loss: 0.1608 - val_matthews_correlation: 0.8850\n",
      "Epoch 72/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1148 - matthews_correlation: 0.9164\n",
      "Epoch 00072: val_matthews_correlation did not improve from 0.88495\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1149 - matthews_correlation: 0.9164 - val_loss: 0.1653 - val_matthews_correlation: 0.8848\n",
      "Epoch 73/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1072 - matthews_correlation: 0.9223\n",
      "Epoch 00073: val_matthews_correlation improved from 0.88495 to 0.88829, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1072 - matthews_correlation: 0.9223 - val_loss: 0.1631 - val_matthews_correlation: 0.8883\n",
      "Epoch 74/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1040 - matthews_correlation: 0.9244\n",
      "Epoch 00074: val_matthews_correlation improved from 0.88829 to 0.89340, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1040 - matthews_correlation: 0.9244 - val_loss: 0.1518 - val_matthews_correlation: 0.8934\n",
      "Epoch 75/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1074 - matthews_correlation: 0.9217\n",
      "Epoch 00075: val_matthews_correlation improved from 0.89340 to 0.89473, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1073 - matthews_correlation: 0.9217 - val_loss: 0.1521 - val_matthews_correlation: 0.8947\n",
      "Epoch 76/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1033 - matthews_correlation: 0.9250\n",
      "Epoch 00076: val_matthews_correlation did not improve from 0.89473\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1033 - matthews_correlation: 0.9250 - val_loss: 0.1682 - val_matthews_correlation: 0.8844\n",
      "Epoch 77/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1011 - matthews_correlation: 0.9269\n",
      "Epoch 00077: val_matthews_correlation improved from 0.89473 to 0.89802, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1011 - matthews_correlation: 0.9269 - val_loss: 0.1544 - val_matthews_correlation: 0.8980\n",
      "Epoch 78/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1001 - matthews_correlation: 0.9272\n",
      "Epoch 00078: val_matthews_correlation did not improve from 0.89802\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1002 - matthews_correlation: 0.9271 - val_loss: 0.1600 - val_matthews_correlation: 0.8919\n",
      "Epoch 79/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0963 - matthews_correlation: 0.9301\n",
      "Epoch 00079: val_matthews_correlation did not improve from 0.89802\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0964 - matthews_correlation: 0.9300 - val_loss: 0.1617 - val_matthews_correlation: 0.8893\n",
      "Epoch 80/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0995 - matthews_correlation: 0.9278\n",
      "Epoch 00080: val_matthews_correlation did not improve from 0.89802\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0995 - matthews_correlation: 0.9278 - val_loss: 0.1756 - val_matthews_correlation: 0.8835\n",
      "Epoch 81/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1031 - matthews_correlation: 0.9250\n",
      "Epoch 00081: val_matthews_correlation did not improve from 0.89802\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1031 - matthews_correlation: 0.9250 - val_loss: 0.1568 - val_matthews_correlation: 0.8926\n",
      "Epoch 82/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0964 - matthews_correlation: 0.9307\n",
      "Epoch 00082: val_matthews_correlation improved from 0.89802 to 0.90117, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0963 - matthews_correlation: 0.9308 - val_loss: 0.1498 - val_matthews_correlation: 0.9012\n",
      "Epoch 83/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0926 - matthews_correlation: 0.9336\n",
      "Epoch 00083: val_matthews_correlation did not improve from 0.90117\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0927 - matthews_correlation: 0.9336 - val_loss: 0.1556 - val_matthews_correlation: 0.8952\n",
      "Epoch 84/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0954 - matthews_correlation: 0.9311\n",
      "Epoch 00084: val_matthews_correlation did not improve from 0.90117\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0954 - matthews_correlation: 0.9310 - val_loss: 0.1747 - val_matthews_correlation: 0.8806\n",
      "Epoch 85/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0939 - matthews_correlation: 0.9318\n",
      "Epoch 00085: val_matthews_correlation did not improve from 0.90117\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0939 - matthews_correlation: 0.9318 - val_loss: 0.1477 - val_matthews_correlation: 0.9007\n",
      "Epoch 86/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0913 - matthews_correlation: 0.9338\n",
      "Epoch 00086: val_matthews_correlation did not improve from 0.90117\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0916 - matthews_correlation: 0.9336 - val_loss: 0.1793 - val_matthews_correlation: 0.8807\n",
      "Epoch 87/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0954 - matthews_correlation: 0.9312\n",
      "Epoch 00087: val_matthews_correlation did not improve from 0.90117\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0954 - matthews_correlation: 0.9312 - val_loss: 0.1577 - val_matthews_correlation: 0.8984\n",
      "Epoch 88/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0912 - matthews_correlation: 0.9347\n",
      "Epoch 00088: val_matthews_correlation did not improve from 0.90117\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0911 - matthews_correlation: 0.9347 - val_loss: 0.1525 - val_matthews_correlation: 0.8943\n",
      "Epoch 89/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0862 - matthews_correlation: 0.9386\n",
      "Epoch 00089: val_matthews_correlation improved from 0.90117 to 0.90191, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0862 - matthews_correlation: 0.9386 - val_loss: 0.1494 - val_matthews_correlation: 0.9019\n",
      "Epoch 90/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0856 - matthews_correlation: 0.9381\n",
      "Epoch 00090: val_matthews_correlation did not improve from 0.90191\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0856 - matthews_correlation: 0.9380 - val_loss: 0.1537 - val_matthews_correlation: 0.8992\n",
      "Epoch 91/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0863 - matthews_correlation: 0.9380\n",
      "Epoch 00091: val_matthews_correlation did not improve from 0.90191\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.0863 - matthews_correlation: 0.9380 - val_loss: 0.1527 - val_matthews_correlation: 0.9013\n",
      "Epoch 92/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0883 - matthews_correlation: 0.9365\n",
      "Epoch 00092: val_matthews_correlation did not improve from 0.90191\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0883 - matthews_correlation: 0.9365 - val_loss: 0.1639 - val_matthews_correlation: 0.8922\n",
      "Epoch 93/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0920 - matthews_correlation: 0.9327\n",
      "Epoch 00093: val_matthews_correlation did not improve from 0.90191\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0920 - matthews_correlation: 0.9327 - val_loss: 0.1603 - val_matthews_correlation: 0.8935\n",
      "Epoch 94/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0886 - matthews_correlation: 0.9359\n",
      "Epoch 00094: val_matthews_correlation did not improve from 0.90191\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.0886 - matthews_correlation: 0.9360 - val_loss: 0.1581 - val_matthews_correlation: 0.8971\n",
      "Epoch 95/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0815 - matthews_correlation: 0.9412\n",
      "Epoch 00095: val_matthews_correlation improved from 0.90191 to 0.90310, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0815 - matthews_correlation: 0.9411 - val_loss: 0.1497 - val_matthews_correlation: 0.9031\n",
      "Epoch 96/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0796 - matthews_correlation: 0.9428\n",
      "Epoch 00096: val_matthews_correlation improved from 0.90310 to 0.90632, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0796 - matthews_correlation: 0.9428 - val_loss: 0.1497 - val_matthews_correlation: 0.9063\n",
      "Epoch 97/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0792 - matthews_correlation: 0.9442\n",
      "Epoch 00097: val_matthews_correlation did not improve from 0.90632\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0791 - matthews_correlation: 0.9443 - val_loss: 0.1544 - val_matthews_correlation: 0.9020\n",
      "Epoch 98/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0785 - matthews_correlation: 0.9446\n",
      "Epoch 00098: val_matthews_correlation improved from 0.90632 to 0.91108, saving model to weights_1.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.0785 - matthews_correlation: 0.9446 - val_loss: 0.1414 - val_matthews_correlation: 0.9111\n",
      "Epoch 99/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0773 - matthews_correlation: 0.9450\n",
      "Epoch 00099: val_matthews_correlation did not improve from 0.91108\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0773 - matthews_correlation: 0.9449 - val_loss: 0.1520 - val_matthews_correlation: 0.9027\n",
      "Epoch 100/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0787 - matthews_correlation: 0.9437\n",
      "Epoch 00100: val_matthews_correlation did not improve from 0.91108\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.0787 - matthews_correlation: 0.9436 - val_loss: 0.1552 - val_matthews_correlation: 0.9001\n",
      "Beginning fold 3\n",
      "Train on 239389 samples, validate on 59847 samples\n",
      "Epoch 1/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.6190 - matthews_correlation: 0.3271\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.34033, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.6190 - matthews_correlation: 0.3271 - val_loss: 0.6144 - val_matthews_correlation: 0.3403\n",
      "Epoch 2/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.6056 - matthews_correlation: 0.3505\n",
      "Epoch 00002: val_matthews_correlation improved from 0.34033 to 0.36062, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.6055 - matthews_correlation: 0.3506 - val_loss: 0.5941 - val_matthews_correlation: 0.3606\n",
      "Epoch 3/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.5914 - matthews_correlation: 0.3627\n",
      "Epoch 00003: val_matthews_correlation improved from 0.36062 to 0.37007, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5915 - matthews_correlation: 0.3623 - val_loss: 0.5857 - val_matthews_correlation: 0.3701\n",
      "Epoch 4/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.5797 - matthews_correlation: 0.3758\n",
      "Epoch 00004: val_matthews_correlation improved from 0.37007 to 0.37954, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5796 - matthews_correlation: 0.3758 - val_loss: 0.5755 - val_matthews_correlation: 0.3795\n",
      "Epoch 5/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.5653 - matthews_correlation: 0.3920\n",
      "Epoch 00005: val_matthews_correlation improved from 0.37954 to 0.40778, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5653 - matthews_correlation: 0.3920 - val_loss: 0.5573 - val_matthews_correlation: 0.4078\n",
      "Epoch 6/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.5499 - matthews_correlation: 0.4213\n",
      "Epoch 00006: val_matthews_correlation improved from 0.40778 to 0.43093, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5499 - matthews_correlation: 0.4213 - val_loss: 0.5410 - val_matthews_correlation: 0.4309\n",
      "Epoch 7/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.5302 - matthews_correlation: 0.4521\n",
      "Epoch 00007: val_matthews_correlation improved from 0.43093 to 0.46890, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5300 - matthews_correlation: 0.4525 - val_loss: 0.5174 - val_matthews_correlation: 0.4689\n",
      "Epoch 8/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.5077 - matthews_correlation: 0.4840\n",
      "Epoch 00008: val_matthews_correlation improved from 0.46890 to 0.50529, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.5076 - matthews_correlation: 0.4843 - val_loss: 0.4973 - val_matthews_correlation: 0.5053\n",
      "Epoch 9/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.4871 - matthews_correlation: 0.5178\n",
      "Epoch 00009: val_matthews_correlation improved from 0.50529 to 0.52890, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.4869 - matthews_correlation: 0.5181 - val_loss: 0.4815 - val_matthews_correlation: 0.5289\n",
      "Epoch 10/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.4670 - matthews_correlation: 0.5455\n",
      "Epoch 00010: val_matthews_correlation improved from 0.52890 to 0.54804, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.4671 - matthews_correlation: 0.5454 - val_loss: 0.4644 - val_matthews_correlation: 0.5480\n",
      "Epoch 11/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.4460 - matthews_correlation: 0.5724\n",
      "Epoch 00011: val_matthews_correlation improved from 0.54804 to 0.57732, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.4459 - matthews_correlation: 0.5724 - val_loss: 0.4411 - val_matthews_correlation: 0.5773\n",
      "Epoch 12/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.4277 - matthews_correlation: 0.5982\n",
      "Epoch 00012: val_matthews_correlation improved from 0.57732 to 0.60422, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.4276 - matthews_correlation: 0.5983 - val_loss: 0.4288 - val_matthews_correlation: 0.6042\n",
      "Epoch 13/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.4089 - matthews_correlation: 0.6231\n",
      "Epoch 00013: val_matthews_correlation did not improve from 0.60422\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.4088 - matthews_correlation: 0.6231 - val_loss: 0.4219 - val_matthews_correlation: 0.5971\n",
      "Epoch 14/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3945 - matthews_correlation: 0.6412\n",
      "Epoch 00014: val_matthews_correlation improved from 0.60422 to 0.64947, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3944 - matthews_correlation: 0.6414 - val_loss: 0.3913 - val_matthews_correlation: 0.6495\n",
      "Epoch 15/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3780 - matthews_correlation: 0.6617\n",
      "Epoch 00015: val_matthews_correlation improved from 0.64947 to 0.66068, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.3780 - matthews_correlation: 0.6617 - val_loss: 0.3782 - val_matthews_correlation: 0.6607\n",
      "Epoch 16/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3645 - matthews_correlation: 0.6775\n",
      "Epoch 00016: val_matthews_correlation improved from 0.66068 to 0.67613, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3645 - matthews_correlation: 0.6776 - val_loss: 0.3682 - val_matthews_correlation: 0.6761\n",
      "Epoch 17/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3541 - matthews_correlation: 0.6894\n",
      "Epoch 00017: val_matthews_correlation improved from 0.67613 to 0.68317, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3540 - matthews_correlation: 0.6895 - val_loss: 0.3614 - val_matthews_correlation: 0.6832\n",
      "Epoch 18/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3354 - matthews_correlation: 0.7108\n",
      "Epoch 00018: val_matthews_correlation improved from 0.68317 to 0.70372, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3353 - matthews_correlation: 0.7110 - val_loss: 0.3399 - val_matthews_correlation: 0.7037\n",
      "Epoch 19/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3256 - matthews_correlation: 0.7218\n",
      "Epoch 00019: val_matthews_correlation improved from 0.70372 to 0.71942, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.3256 - matthews_correlation: 0.7219 - val_loss: 0.3322 - val_matthews_correlation: 0.7194\n",
      "Epoch 20/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3147 - matthews_correlation: 0.7335\n",
      "Epoch 00020: val_matthews_correlation improved from 0.71942 to 0.72504, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.3147 - matthews_correlation: 0.7335 - val_loss: 0.3275 - val_matthews_correlation: 0.7250\n",
      "Epoch 21/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3023 - matthews_correlation: 0.7483\n",
      "Epoch 00021: val_matthews_correlation improved from 0.72504 to 0.73051, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.3023 - matthews_correlation: 0.7483 - val_loss: 0.3162 - val_matthews_correlation: 0.7305\n",
      "Epoch 22/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2919 - matthews_correlation: 0.7584\n",
      "Epoch 00022: val_matthews_correlation improved from 0.73051 to 0.75560, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.2919 - matthews_correlation: 0.7585 - val_loss: 0.2972 - val_matthews_correlation: 0.7556\n",
      "Epoch 23/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2840 - matthews_correlation: 0.7662\n",
      "Epoch 00023: val_matthews_correlation improved from 0.75560 to 0.75853, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.2839 - matthews_correlation: 0.7663 - val_loss: 0.2952 - val_matthews_correlation: 0.7585\n",
      "Epoch 24/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2760 - matthews_correlation: 0.7741\n",
      "Epoch 00024: val_matthews_correlation improved from 0.75853 to 0.76810, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.2760 - matthews_correlation: 0.7741 - val_loss: 0.2856 - val_matthews_correlation: 0.7681\n",
      "Epoch 25/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2666 - matthews_correlation: 0.7831\n",
      "Epoch 00025: val_matthews_correlation improved from 0.76810 to 0.77603, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.2667 - matthews_correlation: 0.7830 - val_loss: 0.2771 - val_matthews_correlation: 0.7760\n",
      "Epoch 26/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2596 - matthews_correlation: 0.7892\n",
      "Epoch 00026: val_matthews_correlation improved from 0.77603 to 0.77797, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.2596 - matthews_correlation: 0.7892 - val_loss: 0.2781 - val_matthews_correlation: 0.7780\n",
      "Epoch 27/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.2494 - matthews_correlation: 0.8010\n",
      "Epoch 00027: val_matthews_correlation did not improve from 0.77797\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2496 - matthews_correlation: 0.8006 - val_loss: 0.2783 - val_matthews_correlation: 0.7756\n",
      "Epoch 28/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2443 - matthews_correlation: 0.8047\n",
      "Epoch 00028: val_matthews_correlation improved from 0.77797 to 0.79332, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2444 - matthews_correlation: 0.8046 - val_loss: 0.2635 - val_matthews_correlation: 0.7933\n",
      "Epoch 29/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2396 - matthews_correlation: 0.8087\n",
      "Epoch 00029: val_matthews_correlation improved from 0.79332 to 0.80125, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.2394 - matthews_correlation: 0.8089 - val_loss: 0.2530 - val_matthews_correlation: 0.8012\n",
      "Epoch 30/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2311 - matthews_correlation: 0.8168\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.80125\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.2311 - matthews_correlation: 0.8167 - val_loss: 0.2509 - val_matthews_correlation: 0.7994\n",
      "Epoch 31/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2274 - matthews_correlation: 0.8197\n",
      "Epoch 00031: val_matthews_correlation improved from 0.80125 to 0.80553, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.2274 - matthews_correlation: 0.8197 - val_loss: 0.2478 - val_matthews_correlation: 0.8055\n",
      "Epoch 32/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.2221 - matthews_correlation: 0.8244\n",
      "Epoch 00032: val_matthews_correlation improved from 0.80553 to 0.80894, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.2220 - matthews_correlation: 0.8245 - val_loss: 0.2460 - val_matthews_correlation: 0.8089\n",
      "Epoch 33/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2142 - matthews_correlation: 0.8321\n",
      "Epoch 00033: val_matthews_correlation did not improve from 0.80894\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.2142 - matthews_correlation: 0.8321 - val_loss: 0.2422 - val_matthews_correlation: 0.8070\n",
      "Epoch 34/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2109 - matthews_correlation: 0.8350\n",
      "Epoch 00034: val_matthews_correlation improved from 0.80894 to 0.81257, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.2108 - matthews_correlation: 0.8350 - val_loss: 0.2397 - val_matthews_correlation: 0.8126\n",
      "Epoch 35/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2050 - matthews_correlation: 0.8408\n",
      "Epoch 00035: val_matthews_correlation improved from 0.81257 to 0.81960, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.2051 - matthews_correlation: 0.8407 - val_loss: 0.2350 - val_matthews_correlation: 0.8196\n",
      "Epoch 36/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2022 - matthews_correlation: 0.8430\n",
      "Epoch 00036: val_matthews_correlation did not improve from 0.81960\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.2022 - matthews_correlation: 0.8431 - val_loss: 0.2353 - val_matthews_correlation: 0.8180\n",
      "Epoch 37/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1989 - matthews_correlation: 0.8456\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.81960\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1988 - matthews_correlation: 0.8458 - val_loss: 0.2357 - val_matthews_correlation: 0.8194\n",
      "Epoch 38/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1936 - matthews_correlation: 0.8509\n",
      "Epoch 00038: val_matthews_correlation improved from 0.81960 to 0.82700, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1936 - matthews_correlation: 0.8510 - val_loss: 0.2224 - val_matthews_correlation: 0.8270\n",
      "Epoch 39/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1923 - matthews_correlation: 0.8522\n",
      "Epoch 00039: val_matthews_correlation improved from 0.82700 to 0.83558, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1922 - matthews_correlation: 0.8523 - val_loss: 0.2186 - val_matthews_correlation: 0.8356\n",
      "Epoch 40/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1866 - matthews_correlation: 0.8560\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.83558\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1866 - matthews_correlation: 0.8561 - val_loss: 0.2223 - val_matthews_correlation: 0.8329\n",
      "Epoch 41/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1826 - matthews_correlation: 0.8609\n",
      "Epoch 00041: val_matthews_correlation improved from 0.83558 to 0.83881, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1826 - matthews_correlation: 0.8609 - val_loss: 0.2111 - val_matthews_correlation: 0.8388\n",
      "Epoch 42/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1773 - matthews_correlation: 0.8654\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.83881\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1774 - matthews_correlation: 0.8653 - val_loss: 0.2189 - val_matthews_correlation: 0.8299\n",
      "Epoch 43/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1748 - matthews_correlation: 0.8670\n",
      "Epoch 00043: val_matthews_correlation improved from 0.83881 to 0.84065, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1748 - matthews_correlation: 0.8669 - val_loss: 0.2133 - val_matthews_correlation: 0.8406\n",
      "Epoch 44/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1734 - matthews_correlation: 0.8683\n",
      "Epoch 00044: val_matthews_correlation improved from 0.84065 to 0.85185, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1734 - matthews_correlation: 0.8683 - val_loss: 0.1974 - val_matthews_correlation: 0.8519\n",
      "Epoch 45/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1684 - matthews_correlation: 0.8734\n",
      "Epoch 00045: val_matthews_correlation did not improve from 0.85185\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1686 - matthews_correlation: 0.8733 - val_loss: 0.2039 - val_matthews_correlation: 0.8441\n",
      "Epoch 46/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1643 - matthews_correlation: 0.8756\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.85185\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1644 - matthews_correlation: 0.8755 - val_loss: 0.2099 - val_matthews_correlation: 0.8434\n",
      "Epoch 47/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1604 - matthews_correlation: 0.8796\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.85185\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1606 - matthews_correlation: 0.8795 - val_loss: 0.1999 - val_matthews_correlation: 0.8477\n",
      "Epoch 48/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1595 - matthews_correlation: 0.8812\n",
      "Epoch 00048: val_matthews_correlation improved from 0.85185 to 0.86120, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1594 - matthews_correlation: 0.8813 - val_loss: 0.1919 - val_matthews_correlation: 0.8612\n",
      "Epoch 49/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1555 - matthews_correlation: 0.8835\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.86120\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1555 - matthews_correlation: 0.8835 - val_loss: 0.1891 - val_matthews_correlation: 0.8591\n",
      "Epoch 50/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1524 - matthews_correlation: 0.8863\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.86120\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1524 - matthews_correlation: 0.8863 - val_loss: 0.1866 - val_matthews_correlation: 0.8611\n",
      "Epoch 51/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1530 - matthews_correlation: 0.8852\n",
      "Epoch 00051: val_matthews_correlation did not improve from 0.86120\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1529 - matthews_correlation: 0.8852 - val_loss: 0.1903 - val_matthews_correlation: 0.8589\n",
      "Epoch 52/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1480 - matthews_correlation: 0.8889\n",
      "Epoch 00052: val_matthews_correlation did not improve from 0.86120\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1479 - matthews_correlation: 0.8890 - val_loss: 0.1972 - val_matthews_correlation: 0.8528\n",
      "Epoch 53/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1468 - matthews_correlation: 0.8901\n",
      "Epoch 00053: val_matthews_correlation improved from 0.86120 to 0.86156, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1468 - matthews_correlation: 0.8901 - val_loss: 0.1880 - val_matthews_correlation: 0.8616\n",
      "Epoch 54/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1441 - matthews_correlation: 0.8934\n",
      "Epoch 00054: val_matthews_correlation improved from 0.86156 to 0.86168, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1440 - matthews_correlation: 0.8935 - val_loss: 0.1899 - val_matthews_correlation: 0.8617\n",
      "Epoch 55/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1407 - matthews_correlation: 0.8960\n",
      "Epoch 00055: val_matthews_correlation did not improve from 0.86168\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1407 - matthews_correlation: 0.8959 - val_loss: 0.1878 - val_matthews_correlation: 0.8593\n",
      "Epoch 56/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1401 - matthews_correlation: 0.8958\n",
      "Epoch 00056: val_matthews_correlation improved from 0.86168 to 0.87073, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1401 - matthews_correlation: 0.8960 - val_loss: 0.1776 - val_matthews_correlation: 0.8707\n",
      "Epoch 57/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1360 - matthews_correlation: 0.8993\n",
      "Epoch 00057: val_matthews_correlation improved from 0.87073 to 0.87633, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1360 - matthews_correlation: 0.8994 - val_loss: 0.1679 - val_matthews_correlation: 0.8763\n",
      "Epoch 58/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1359 - matthews_correlation: 0.8991\n",
      "Epoch 00058: val_matthews_correlation did not improve from 0.87633\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1359 - matthews_correlation: 0.8991 - val_loss: 0.1814 - val_matthews_correlation: 0.8658\n",
      "Epoch 59/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1305 - matthews_correlation: 0.9042\n",
      "Epoch 00059: val_matthews_correlation did not improve from 0.87633\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1305 - matthews_correlation: 0.9042 - val_loss: 0.1808 - val_matthews_correlation: 0.8710\n",
      "Epoch 60/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1347 - matthews_correlation: 0.8994\n",
      "Epoch 00060: val_matthews_correlation improved from 0.87633 to 0.88332, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1348 - matthews_correlation: 0.8993 - val_loss: 0.1663 - val_matthews_correlation: 0.8833\n",
      "Epoch 61/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1291 - matthews_correlation: 0.9049\n",
      "Epoch 00061: val_matthews_correlation did not improve from 0.88332\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1291 - matthews_correlation: 0.9049 - val_loss: 0.1755 - val_matthews_correlation: 0.8738\n",
      "Epoch 62/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1239 - matthews_correlation: 0.9092\n",
      "Epoch 00062: val_matthews_correlation did not improve from 0.88332\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1240 - matthews_correlation: 0.9092 - val_loss: 0.1719 - val_matthews_correlation: 0.8824\n",
      "Epoch 63/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1246 - matthews_correlation: 0.9085\n",
      "Epoch 00063: val_matthews_correlation did not improve from 0.88332\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1246 - matthews_correlation: 0.9085 - val_loss: 0.1733 - val_matthews_correlation: 0.8774\n",
      "Epoch 64/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1244 - matthews_correlation: 0.9088\n",
      "Epoch 00064: val_matthews_correlation did not improve from 0.88332\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1244 - matthews_correlation: 0.9087 - val_loss: 0.1706 - val_matthews_correlation: 0.8780\n",
      "Epoch 65/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1226 - matthews_correlation: 0.9100\n",
      "Epoch 00065: val_matthews_correlation did not improve from 0.88332\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1226 - matthews_correlation: 0.9101 - val_loss: 0.1785 - val_matthews_correlation: 0.8741\n",
      "Epoch 66/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1185 - matthews_correlation: 0.9134\n",
      "Epoch 00066: val_matthews_correlation did not improve from 0.88332\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1185 - matthews_correlation: 0.9134 - val_loss: 0.1673 - val_matthews_correlation: 0.8805\n",
      "Epoch 67/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1178 - matthews_correlation: 0.9135\n",
      "Epoch 00067: val_matthews_correlation did not improve from 0.88332\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1177 - matthews_correlation: 0.9136 - val_loss: 0.1711 - val_matthews_correlation: 0.8816\n",
      "Epoch 68/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1169 - matthews_correlation: 0.9146\n",
      "Epoch 00068: val_matthews_correlation did not improve from 0.88332\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1169 - matthews_correlation: 0.9146 - val_loss: 0.1776 - val_matthews_correlation: 0.8766\n",
      "Epoch 69/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1161 - matthews_correlation: 0.9144\n",
      "Epoch 00069: val_matthews_correlation did not improve from 0.88332\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1160 - matthews_correlation: 0.9145 - val_loss: 0.1758 - val_matthews_correlation: 0.8800\n",
      "Epoch 70/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1137 - matthews_correlation: 0.9162\n",
      "Epoch 00070: val_matthews_correlation did not improve from 0.88332\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1137 - matthews_correlation: 0.9161 - val_loss: 0.1833 - val_matthews_correlation: 0.8666\n",
      "Epoch 71/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1127 - matthews_correlation: 0.9186\n",
      "Epoch 00071: val_matthews_correlation did not improve from 0.88332\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1129 - matthews_correlation: 0.9186 - val_loss: 0.1781 - val_matthews_correlation: 0.8748\n",
      "Epoch 72/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1115 - matthews_correlation: 0.9185\n",
      "Epoch 00072: val_matthews_correlation improved from 0.88332 to 0.88430, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1115 - matthews_correlation: 0.9185 - val_loss: 0.1656 - val_matthews_correlation: 0.8843\n",
      "Epoch 73/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1105 - matthews_correlation: 0.9191\n",
      "Epoch 00073: val_matthews_correlation improved from 0.88430 to 0.89005, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.1105 - matthews_correlation: 0.9190 - val_loss: 0.1599 - val_matthews_correlation: 0.8901\n",
      "Epoch 74/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1066 - matthews_correlation: 0.9231\n",
      "Epoch 00074: val_matthews_correlation did not improve from 0.89005\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1066 - matthews_correlation: 0.9230 - val_loss: 0.1625 - val_matthews_correlation: 0.8869\n",
      "Epoch 75/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1123 - matthews_correlation: 0.9187\n",
      "Epoch 00075: val_matthews_correlation did not improve from 0.89005\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1124 - matthews_correlation: 0.9186 - val_loss: 0.1857 - val_matthews_correlation: 0.8717\n",
      "Epoch 76/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1087 - matthews_correlation: 0.9204\n",
      "Epoch 00076: val_matthews_correlation did not improve from 0.89005\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1087 - matthews_correlation: 0.9204 - val_loss: 0.1590 - val_matthews_correlation: 0.8885\n",
      "Epoch 77/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1006 - matthews_correlation: 0.9270\n",
      "Epoch 00077: val_matthews_correlation improved from 0.89005 to 0.89705, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1006 - matthews_correlation: 0.9270 - val_loss: 0.1544 - val_matthews_correlation: 0.8971\n",
      "Epoch 78/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1008 - matthews_correlation: 0.9266\n",
      "Epoch 00078: val_matthews_correlation did not improve from 0.89705\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1008 - matthews_correlation: 0.9266 - val_loss: 0.1685 - val_matthews_correlation: 0.8859\n",
      "Epoch 79/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1024 - matthews_correlation: 0.9251\n",
      "Epoch 00079: val_matthews_correlation did not improve from 0.89705\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1024 - matthews_correlation: 0.9251 - val_loss: 0.1584 - val_matthews_correlation: 0.8916\n",
      "Epoch 80/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0950 - matthews_correlation: 0.9325\n",
      "Epoch 00080: val_matthews_correlation did not improve from 0.89705\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0949 - matthews_correlation: 0.9325 - val_loss: 0.1608 - val_matthews_correlation: 0.8887\n",
      "Epoch 81/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1016 - matthews_correlation: 0.9257\n",
      "Epoch 00081: val_matthews_correlation did not improve from 0.89705\n",
      "239389/239389 [==============================] - 7s 27us/sample - loss: 0.1018 - matthews_correlation: 0.9257 - val_loss: 0.1581 - val_matthews_correlation: 0.8900\n",
      "Epoch 82/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0951 - matthews_correlation: 0.9319\n",
      "Epoch 00082: val_matthews_correlation did not improve from 0.89705\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0951 - matthews_correlation: 0.9320 - val_loss: 0.1520 - val_matthews_correlation: 0.8970\n",
      "Epoch 83/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1021 - matthews_correlation: 0.9261\n",
      "Epoch 00083: val_matthews_correlation did not improve from 0.89705\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1021 - matthews_correlation: 0.9262 - val_loss: 0.1556 - val_matthews_correlation: 0.8913\n",
      "Epoch 84/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0938 - matthews_correlation: 0.9316\n",
      "Epoch 00084: val_matthews_correlation did not improve from 0.89705\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0938 - matthews_correlation: 0.9316 - val_loss: 0.1610 - val_matthews_correlation: 0.8942\n",
      "Epoch 85/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0914 - matthews_correlation: 0.9341\n",
      "Epoch 00085: val_matthews_correlation did not improve from 0.89705\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0914 - matthews_correlation: 0.9341 - val_loss: 0.1549 - val_matthews_correlation: 0.8949\n",
      "Epoch 86/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0942 - matthews_correlation: 0.9320\n",
      "Epoch 00086: val_matthews_correlation improved from 0.89705 to 0.90108, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.0942 - matthews_correlation: 0.9319 - val_loss: 0.1509 - val_matthews_correlation: 0.9011\n",
      "Epoch 87/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0918 - matthews_correlation: 0.9342\n",
      "Epoch 00087: val_matthews_correlation did not improve from 0.90108\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.0918 - matthews_correlation: 0.9342 - val_loss: 0.1541 - val_matthews_correlation: 0.8996\n",
      "Epoch 88/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0891 - matthews_correlation: 0.9362\n",
      "Epoch 00088: val_matthews_correlation did not improve from 0.90108\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0892 - matthews_correlation: 0.9360 - val_loss: 0.1564 - val_matthews_correlation: 0.8941\n",
      "Epoch 89/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0896 - matthews_correlation: 0.9356\n",
      "Epoch 00089: val_matthews_correlation did not improve from 0.90108\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0896 - matthews_correlation: 0.9355 - val_loss: 0.1566 - val_matthews_correlation: 0.8952\n",
      "Epoch 90/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0923 - matthews_correlation: 0.9341\n",
      "Epoch 00090: val_matthews_correlation improved from 0.90108 to 0.90239, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.0922 - matthews_correlation: 0.9342 - val_loss: 0.1457 - val_matthews_correlation: 0.9024\n",
      "Epoch 91/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0845 - matthews_correlation: 0.9389\n",
      "Epoch 00091: val_matthews_correlation improved from 0.90239 to 0.90627, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0844 - matthews_correlation: 0.9390 - val_loss: 0.1447 - val_matthews_correlation: 0.9063\n",
      "Epoch 92/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0870 - matthews_correlation: 0.9373\n",
      "Epoch 00092: val_matthews_correlation did not improve from 0.90627\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0872 - matthews_correlation: 0.9371 - val_loss: 0.1805 - val_matthews_correlation: 0.8822\n",
      "Epoch 93/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0891 - matthews_correlation: 0.9358\n",
      "Epoch 00093: val_matthews_correlation did not improve from 0.90627\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0891 - matthews_correlation: 0.9358 - val_loss: 0.1477 - val_matthews_correlation: 0.9034\n",
      "Epoch 94/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0795 - matthews_correlation: 0.9435\n",
      "Epoch 00094: val_matthews_correlation did not improve from 0.90627\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0795 - matthews_correlation: 0.9434 - val_loss: 0.1578 - val_matthews_correlation: 0.9000\n",
      "Epoch 95/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0857 - matthews_correlation: 0.9387\n",
      "Epoch 00095: val_matthews_correlation did not improve from 0.90627\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0856 - matthews_correlation: 0.9387 - val_loss: 0.1573 - val_matthews_correlation: 0.8982\n",
      "Epoch 96/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0883 - matthews_correlation: 0.9372\n",
      "Epoch 00096: val_matthews_correlation improved from 0.90627 to 0.90639, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0883 - matthews_correlation: 0.9372 - val_loss: 0.1453 - val_matthews_correlation: 0.9064\n",
      "Epoch 97/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0835 - matthews_correlation: 0.9404\n",
      "Epoch 00097: val_matthews_correlation improved from 0.90639 to 0.90733, saving model to weights_2.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.0836 - matthews_correlation: 0.9404 - val_loss: 0.1426 - val_matthews_correlation: 0.9073\n",
      "Epoch 98/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0787 - matthews_correlation: 0.9444\n",
      "Epoch 00098: val_matthews_correlation did not improve from 0.90733\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0788 - matthews_correlation: 0.9444 - val_loss: 0.1578 - val_matthews_correlation: 0.9014\n",
      "Epoch 99/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0796 - matthews_correlation: 0.9430\n",
      "Epoch 00099: val_matthews_correlation did not improve from 0.90733\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0797 - matthews_correlation: 0.9430 - val_loss: 0.1453 - val_matthews_correlation: 0.9070\n",
      "Epoch 100/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0795 - matthews_correlation: 0.9438\n",
      "Epoch 00100: val_matthews_correlation did not improve from 0.90733\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0795 - matthews_correlation: 0.9437 - val_loss: 0.1721 - val_matthews_correlation: 0.8898\n",
      "Beginning fold 4\n",
      "Train on 239389 samples, validate on 59847 samples\n",
      "Epoch 1/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.6184 - matthews_correlation: 0.3294\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.34329, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.6184 - matthews_correlation: 0.3294 - val_loss: 0.6090 - val_matthews_correlation: 0.3433\n",
      "Epoch 2/100\n",
      "239389/239389 [==============================] - ETA: 0s - loss: 0.6029 - matthews_correlation: 0.3516\n",
      "Epoch 00002: val_matthews_correlation improved from 0.34329 to 0.35324, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 27us/sample - loss: 0.6029 - matthews_correlation: 0.3516 - val_loss: 0.5956 - val_matthews_correlation: 0.3532\n",
      "Epoch 3/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.5904 - matthews_correlation: 0.3636\n",
      "Epoch 00003: val_matthews_correlation improved from 0.35324 to 0.37155, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5903 - matthews_correlation: 0.3636 - val_loss: 0.5836 - val_matthews_correlation: 0.3716\n",
      "Epoch 4/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.5784 - matthews_correlation: 0.3786\n",
      "Epoch 00004: val_matthews_correlation improved from 0.37155 to 0.39532, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.5784 - matthews_correlation: 0.3788 - val_loss: 0.5704 - val_matthews_correlation: 0.3953\n",
      "Epoch 5/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.5635 - matthews_correlation: 0.3981\n",
      "Epoch 00005: val_matthews_correlation improved from 0.39532 to 0.41725, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5635 - matthews_correlation: 0.3981 - val_loss: 0.5512 - val_matthews_correlation: 0.4173\n",
      "Epoch 6/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.5498 - matthews_correlation: 0.4211\n",
      "Epoch 00006: val_matthews_correlation improved from 0.41725 to 0.43973, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5498 - matthews_correlation: 0.4212 - val_loss: 0.5413 - val_matthews_correlation: 0.4397\n",
      "Epoch 7/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.5308 - matthews_correlation: 0.4518\n",
      "Epoch 00007: val_matthews_correlation improved from 0.43973 to 0.45726, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5308 - matthews_correlation: 0.4516 - val_loss: 0.5240 - val_matthews_correlation: 0.4573\n",
      "Epoch 8/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.5130 - matthews_correlation: 0.4793\n",
      "Epoch 00008: val_matthews_correlation improved from 0.45726 to 0.49998, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5130 - matthews_correlation: 0.4794 - val_loss: 0.5021 - val_matthews_correlation: 0.5000\n",
      "Epoch 9/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.4947 - matthews_correlation: 0.5059\n",
      "Epoch 00009: val_matthews_correlation improved from 0.49998 to 0.52433, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.4946 - matthews_correlation: 0.5059 - val_loss: 0.4868 - val_matthews_correlation: 0.5243\n",
      "Epoch 10/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.4748 - matthews_correlation: 0.5339\n",
      "Epoch 00010: val_matthews_correlation improved from 0.52433 to 0.54150, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.4748 - matthews_correlation: 0.5340 - val_loss: 0.4684 - val_matthews_correlation: 0.5415\n",
      "Epoch 11/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.4537 - matthews_correlation: 0.5640\n",
      "Epoch 00011: val_matthews_correlation improved from 0.54150 to 0.58385, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.4539 - matthews_correlation: 0.5638 - val_loss: 0.4386 - val_matthews_correlation: 0.5839\n",
      "Epoch 12/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.4338 - matthews_correlation: 0.5878\n",
      "Epoch 00012: val_matthews_correlation improved from 0.58385 to 0.61084, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.4337 - matthews_correlation: 0.5878 - val_loss: 0.4254 - val_matthews_correlation: 0.6108\n",
      "Epoch 13/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.4168 - matthews_correlation: 0.6126\n",
      "Epoch 00013: val_matthews_correlation improved from 0.61084 to 0.63066, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.4166 - matthews_correlation: 0.6127 - val_loss: 0.4062 - val_matthews_correlation: 0.6307\n",
      "Epoch 14/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3974 - matthews_correlation: 0.6377\n",
      "Epoch 00014: val_matthews_correlation improved from 0.63066 to 0.65265, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3974 - matthews_correlation: 0.6377 - val_loss: 0.3928 - val_matthews_correlation: 0.6526\n",
      "Epoch 15/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3806 - matthews_correlation: 0.6581\n",
      "Epoch 00015: val_matthews_correlation improved from 0.65265 to 0.66316, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3806 - matthews_correlation: 0.6580 - val_loss: 0.3748 - val_matthews_correlation: 0.6632\n",
      "Epoch 16/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.3648 - matthews_correlation: 0.6775\n",
      "Epoch 00016: val_matthews_correlation improved from 0.66316 to 0.68325, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3648 - matthews_correlation: 0.6774 - val_loss: 0.3626 - val_matthews_correlation: 0.6833\n",
      "Epoch 17/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.3485 - matthews_correlation: 0.6986\n",
      "Epoch 00017: val_matthews_correlation improved from 0.68325 to 0.70920, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3485 - matthews_correlation: 0.6987 - val_loss: 0.3466 - val_matthews_correlation: 0.7092\n",
      "Epoch 18/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3343 - matthews_correlation: 0.7123\n",
      "Epoch 00018: val_matthews_correlation improved from 0.70920 to 0.71145, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3344 - matthews_correlation: 0.7123 - val_loss: 0.3416 - val_matthews_correlation: 0.7114\n",
      "Epoch 19/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3217 - matthews_correlation: 0.7271\n",
      "Epoch 00019: val_matthews_correlation improved from 0.71145 to 0.72238, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3217 - matthews_correlation: 0.7272 - val_loss: 0.3249 - val_matthews_correlation: 0.7224\n",
      "Epoch 20/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3100 - matthews_correlation: 0.7388\n",
      "Epoch 00020: val_matthews_correlation improved from 0.72238 to 0.74310, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3099 - matthews_correlation: 0.7389 - val_loss: 0.3099 - val_matthews_correlation: 0.7431\n",
      "Epoch 21/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2999 - matthews_correlation: 0.7510\n",
      "Epoch 00021: val_matthews_correlation improved from 0.74310 to 0.75543, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.3000 - matthews_correlation: 0.7509 - val_loss: 0.2993 - val_matthews_correlation: 0.7554\n",
      "Epoch 22/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.2887 - matthews_correlation: 0.7632\n",
      "Epoch 00022: val_matthews_correlation improved from 0.75543 to 0.75850, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.2888 - matthews_correlation: 0.7630 - val_loss: 0.2962 - val_matthews_correlation: 0.7585\n",
      "Epoch 23/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.2770 - matthews_correlation: 0.7737\n",
      "Epoch 00023: val_matthews_correlation improved from 0.75850 to 0.77308, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2770 - matthews_correlation: 0.7739 - val_loss: 0.2846 - val_matthews_correlation: 0.7731\n",
      "Epoch 24/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2722 - matthews_correlation: 0.7769\n",
      "Epoch 00024: val_matthews_correlation improved from 0.77308 to 0.78067, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2722 - matthews_correlation: 0.7770 - val_loss: 0.2746 - val_matthews_correlation: 0.7807\n",
      "Epoch 25/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2612 - matthews_correlation: 0.7900\n",
      "Epoch 00025: val_matthews_correlation did not improve from 0.78067\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2613 - matthews_correlation: 0.7899 - val_loss: 0.2749 - val_matthews_correlation: 0.7797\n",
      "Epoch 26/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2567 - matthews_correlation: 0.7945\n",
      "Epoch 00026: val_matthews_correlation improved from 0.78067 to 0.79079, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2567 - matthews_correlation: 0.7945 - val_loss: 0.2688 - val_matthews_correlation: 0.7908\n",
      "Epoch 27/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2500 - matthews_correlation: 0.7998\n",
      "Epoch 00027: val_matthews_correlation improved from 0.79079 to 0.79780, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2500 - matthews_correlation: 0.7998 - val_loss: 0.2545 - val_matthews_correlation: 0.7978\n",
      "Epoch 28/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2418 - matthews_correlation: 0.8082\n",
      "Epoch 00028: val_matthews_correlation improved from 0.79780 to 0.80149, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2417 - matthews_correlation: 0.8082 - val_loss: 0.2546 - val_matthews_correlation: 0.8015\n",
      "Epoch 29/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2348 - matthews_correlation: 0.8147\n",
      "Epoch 00029: val_matthews_correlation improved from 0.80149 to 0.80521, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2348 - matthews_correlation: 0.8147 - val_loss: 0.2483 - val_matthews_correlation: 0.8052\n",
      "Epoch 30/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2296 - matthews_correlation: 0.8213\n",
      "Epoch 00030: val_matthews_correlation improved from 0.80521 to 0.80729, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2295 - matthews_correlation: 0.8214 - val_loss: 0.2465 - val_matthews_correlation: 0.8073\n",
      "Epoch 31/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2232 - matthews_correlation: 0.8262\n",
      "Epoch 00031: val_matthews_correlation improved from 0.80729 to 0.81919, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2232 - matthews_correlation: 0.8262 - val_loss: 0.2370 - val_matthews_correlation: 0.8192\n",
      "Epoch 32/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2226 - matthews_correlation: 0.8255\n",
      "Epoch 00032: val_matthews_correlation did not improve from 0.81919\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2225 - matthews_correlation: 0.8256 - val_loss: 0.2384 - val_matthews_correlation: 0.8178\n",
      "Epoch 33/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2149 - matthews_correlation: 0.8328\n",
      "Epoch 00033: val_matthews_correlation improved from 0.81919 to 0.82384, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2148 - matthews_correlation: 0.8330 - val_loss: 0.2287 - val_matthews_correlation: 0.8238\n",
      "Epoch 34/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2099 - matthews_correlation: 0.8366\n",
      "Epoch 00034: val_matthews_correlation improved from 0.82384 to 0.83255, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2100 - matthews_correlation: 0.8365 - val_loss: 0.2215 - val_matthews_correlation: 0.8325\n",
      "Epoch 35/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.2030 - matthews_correlation: 0.8437\n",
      "Epoch 00035: val_matthews_correlation improved from 0.83255 to 0.83362, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2028 - matthews_correlation: 0.8439 - val_loss: 0.2211 - val_matthews_correlation: 0.8336\n",
      "Epoch 36/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2010 - matthews_correlation: 0.8462\n",
      "Epoch 00036: val_matthews_correlation improved from 0.83362 to 0.83738, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2010 - matthews_correlation: 0.8462 - val_loss: 0.2150 - val_matthews_correlation: 0.8374\n",
      "Epoch 37/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1962 - matthews_correlation: 0.8494\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.83738\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1962 - matthews_correlation: 0.8494 - val_loss: 0.2179 - val_matthews_correlation: 0.8319\n",
      "Epoch 38/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1921 - matthews_correlation: 0.8532\n",
      "Epoch 00038: val_matthews_correlation did not improve from 0.83738\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1922 - matthews_correlation: 0.8532 - val_loss: 0.2164 - val_matthews_correlation: 0.8354\n",
      "Epoch 39/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1865 - matthews_correlation: 0.8571\n",
      "Epoch 00039: val_matthews_correlation improved from 0.83738 to 0.83981, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1864 - matthews_correlation: 0.8571 - val_loss: 0.2113 - val_matthews_correlation: 0.8398\n",
      "Epoch 40/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1847 - matthews_correlation: 0.8594\n",
      "Epoch 00040: val_matthews_correlation improved from 0.83981 to 0.84584, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1846 - matthews_correlation: 0.8595 - val_loss: 0.2036 - val_matthews_correlation: 0.8458\n",
      "Epoch 41/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1811 - matthews_correlation: 0.8630\n",
      "Epoch 00041: val_matthews_correlation improved from 0.84584 to 0.84957, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1811 - matthews_correlation: 0.8630 - val_loss: 0.2021 - val_matthews_correlation: 0.8496\n",
      "Epoch 42/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1781 - matthews_correlation: 0.8650\n",
      "Epoch 00042: val_matthews_correlation did not improve from 0.84957\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1781 - matthews_correlation: 0.8650 - val_loss: 0.2111 - val_matthews_correlation: 0.8423\n",
      "Epoch 43/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1755 - matthews_correlation: 0.8673\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.84957\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1755 - matthews_correlation: 0.8673 - val_loss: 0.2092 - val_matthews_correlation: 0.8397\n",
      "Epoch 44/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1707 - matthews_correlation: 0.8719\n",
      "Epoch 00044: val_matthews_correlation improved from 0.84957 to 0.85009, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1708 - matthews_correlation: 0.8719 - val_loss: 0.2014 - val_matthews_correlation: 0.8501\n",
      "Epoch 45/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1658 - matthews_correlation: 0.8757\n",
      "Epoch 00045: val_matthews_correlation improved from 0.85009 to 0.85309, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1659 - matthews_correlation: 0.8756 - val_loss: 0.1975 - val_matthews_correlation: 0.8531\n",
      "Epoch 46/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1655 - matthews_correlation: 0.8760\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.85309\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1655 - matthews_correlation: 0.8759 - val_loss: 0.2031 - val_matthews_correlation: 0.8462\n",
      "Epoch 47/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1611 - matthews_correlation: 0.8795\n",
      "Epoch 00047: val_matthews_correlation improved from 0.85309 to 0.85816, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1611 - matthews_correlation: 0.8795 - val_loss: 0.1907 - val_matthews_correlation: 0.8582\n",
      "Epoch 48/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1631 - matthews_correlation: 0.8773\n",
      "Epoch 00048: val_matthews_correlation did not improve from 0.85816\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1632 - matthews_correlation: 0.8773 - val_loss: 0.1969 - val_matthews_correlation: 0.8535\n",
      "Epoch 49/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1610 - matthews_correlation: 0.8799\n",
      "Epoch 00049: val_matthews_correlation improved from 0.85816 to 0.86021, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1610 - matthews_correlation: 0.8799 - val_loss: 0.1864 - val_matthews_correlation: 0.8602\n",
      "Epoch 50/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1543 - matthews_correlation: 0.8850\n",
      "Epoch 00050: val_matthews_correlation did not improve from 0.86021\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1544 - matthews_correlation: 0.8848 - val_loss: 0.2074 - val_matthews_correlation: 0.8485\n",
      "Epoch 51/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1529 - matthews_correlation: 0.8855\n",
      "Epoch 00051: val_matthews_correlation did not improve from 0.86021\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1530 - matthews_correlation: 0.8854 - val_loss: 0.1932 - val_matthews_correlation: 0.8554\n",
      "Epoch 52/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1493 - matthews_correlation: 0.8887\n",
      "Epoch 00052: val_matthews_correlation improved from 0.86021 to 0.86685, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1493 - matthews_correlation: 0.8886 - val_loss: 0.1850 - val_matthews_correlation: 0.8668\n",
      "Epoch 53/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1461 - matthews_correlation: 0.8917\n",
      "Epoch 00053: val_matthews_correlation did not improve from 0.86685\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1460 - matthews_correlation: 0.8918 - val_loss: 0.1878 - val_matthews_correlation: 0.8666\n",
      "Epoch 54/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1451 - matthews_correlation: 0.8926\n",
      "Epoch 00054: val_matthews_correlation did not improve from 0.86685\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1452 - matthews_correlation: 0.8925 - val_loss: 0.1890 - val_matthews_correlation: 0.8606\n",
      "Epoch 55/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1439 - matthews_correlation: 0.8935\n",
      "Epoch 00055: val_matthews_correlation improved from 0.86685 to 0.87678, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1438 - matthews_correlation: 0.8935 - val_loss: 0.1694 - val_matthews_correlation: 0.8768\n",
      "Epoch 56/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1410 - matthews_correlation: 0.8960\n",
      "Epoch 00056: val_matthews_correlation did not improve from 0.87678\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1412 - matthews_correlation: 0.8959 - val_loss: 0.1744 - val_matthews_correlation: 0.8739\n",
      "Epoch 57/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1416 - matthews_correlation: 0.8951\n",
      "Epoch 00057: val_matthews_correlation did not improve from 0.87678\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1418 - matthews_correlation: 0.8950 - val_loss: 0.1937 - val_matthews_correlation: 0.8574\n",
      "Epoch 58/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1364 - matthews_correlation: 0.8999\n",
      "Epoch 00058: val_matthews_correlation did not improve from 0.87678\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1363 - matthews_correlation: 0.8999 - val_loss: 0.1724 - val_matthews_correlation: 0.8757\n",
      "Epoch 59/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1350 - matthews_correlation: 0.9009\n",
      "Epoch 00059: val_matthews_correlation improved from 0.87678 to 0.87832, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1350 - matthews_correlation: 0.9008 - val_loss: 0.1667 - val_matthews_correlation: 0.8783\n",
      "Epoch 60/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1350 - matthews_correlation: 0.9009\n",
      "Epoch 00060: val_matthews_correlation did not improve from 0.87832\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1350 - matthews_correlation: 0.9009 - val_loss: 0.1706 - val_matthews_correlation: 0.8779\n",
      "Epoch 61/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1293 - matthews_correlation: 0.9053\n",
      "Epoch 00061: val_matthews_correlation improved from 0.87832 to 0.88308, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1294 - matthews_correlation: 0.9052 - val_loss: 0.1604 - val_matthews_correlation: 0.8831\n",
      "Epoch 62/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1269 - matthews_correlation: 0.9070\n",
      "Epoch 00062: val_matthews_correlation improved from 0.88308 to 0.88552, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1270 - matthews_correlation: 0.9069 - val_loss: 0.1649 - val_matthews_correlation: 0.8855\n",
      "Epoch 63/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1267 - matthews_correlation: 0.9073\n",
      "Epoch 00063: val_matthews_correlation did not improve from 0.88552\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1269 - matthews_correlation: 0.9071 - val_loss: 0.1690 - val_matthews_correlation: 0.8770\n",
      "Epoch 64/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1243 - matthews_correlation: 0.9082\n",
      "Epoch 00064: val_matthews_correlation did not improve from 0.88552\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1245 - matthews_correlation: 0.9081 - val_loss: 0.1749 - val_matthews_correlation: 0.8761\n",
      "Epoch 65/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1243 - matthews_correlation: 0.9094\n",
      "Epoch 00065: val_matthews_correlation did not improve from 0.88552\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1243 - matthews_correlation: 0.9093 - val_loss: 0.1712 - val_matthews_correlation: 0.8803\n",
      "Epoch 66/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1229 - matthews_correlation: 0.9095\n",
      "Epoch 00066: val_matthews_correlation improved from 0.88552 to 0.89151, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1227 - matthews_correlation: 0.9097 - val_loss: 0.1552 - val_matthews_correlation: 0.8915\n",
      "Epoch 67/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1199 - matthews_correlation: 0.9122\n",
      "Epoch 00067: val_matthews_correlation improved from 0.89151 to 0.89167, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1198 - matthews_correlation: 0.9123 - val_loss: 0.1574 - val_matthews_correlation: 0.8917\n",
      "Epoch 68/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1189 - matthews_correlation: 0.9127\n",
      "Epoch 00068: val_matthews_correlation did not improve from 0.89167\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1190 - matthews_correlation: 0.9127 - val_loss: 0.1609 - val_matthews_correlation: 0.8872\n",
      "Epoch 69/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1168 - matthews_correlation: 0.9150\n",
      "Epoch 00069: val_matthews_correlation did not improve from 0.89167\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1167 - matthews_correlation: 0.9151 - val_loss: 0.1594 - val_matthews_correlation: 0.8897\n",
      "Epoch 70/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1133 - matthews_correlation: 0.9180\n",
      "Epoch 00070: val_matthews_correlation did not improve from 0.89167\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1133 - matthews_correlation: 0.9180 - val_loss: 0.1782 - val_matthews_correlation: 0.8753\n",
      "Epoch 71/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1173 - matthews_correlation: 0.9145\n",
      "Epoch 00071: val_matthews_correlation did not improve from 0.89167\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1173 - matthews_correlation: 0.9145 - val_loss: 0.1619 - val_matthews_correlation: 0.8910\n",
      "Epoch 72/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1130 - matthews_correlation: 0.9175\n",
      "Epoch 00072: val_matthews_correlation did not improve from 0.89167\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1131 - matthews_correlation: 0.9175 - val_loss: 0.1598 - val_matthews_correlation: 0.8875\n",
      "Epoch 73/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1117 - matthews_correlation: 0.9193\n",
      "Epoch 00073: val_matthews_correlation did not improve from 0.89167\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1118 - matthews_correlation: 0.9191 - val_loss: 0.1572 - val_matthews_correlation: 0.8896\n",
      "Epoch 74/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1096 - matthews_correlation: 0.9218\n",
      "Epoch 00074: val_matthews_correlation did not improve from 0.89167\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1097 - matthews_correlation: 0.9217 - val_loss: 0.1677 - val_matthews_correlation: 0.8811\n",
      "Epoch 75/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1065 - matthews_correlation: 0.9229\n",
      "Epoch 00075: val_matthews_correlation improved from 0.89167 to 0.89713, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1065 - matthews_correlation: 0.9230 - val_loss: 0.1517 - val_matthews_correlation: 0.8971\n",
      "Epoch 76/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1040 - matthews_correlation: 0.9242\n",
      "Epoch 00076: val_matthews_correlation did not improve from 0.89713\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1040 - matthews_correlation: 0.9242 - val_loss: 0.1527 - val_matthews_correlation: 0.8950\n",
      "Epoch 77/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1100 - matthews_correlation: 0.9191\n",
      "Epoch 00077: val_matthews_correlation did not improve from 0.89713\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1101 - matthews_correlation: 0.9190 - val_loss: 0.1726 - val_matthews_correlation: 0.8787\n",
      "Epoch 78/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1048 - matthews_correlation: 0.9234 ETA: 1s - l\n",
      "Epoch 00078: val_matthews_correlation did not improve from 0.89713\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1050 - matthews_correlation: 0.9232 - val_loss: 0.1656 - val_matthews_correlation: 0.8834\n",
      "Epoch 79/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1011 - matthews_correlation: 0.9275\n",
      "Epoch 00079: val_matthews_correlation improved from 0.89713 to 0.89825, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1010 - matthews_correlation: 0.9275 - val_loss: 0.1494 - val_matthews_correlation: 0.8983\n",
      "Epoch 80/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1018 - matthews_correlation: 0.9261\n",
      "Epoch 00080: val_matthews_correlation improved from 0.89825 to 0.89976, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1019 - matthews_correlation: 0.9260 - val_loss: 0.1461 - val_matthews_correlation: 0.8998\n",
      "Epoch 81/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0997 - matthews_correlation: 0.9282\n",
      "Epoch 00081: val_matthews_correlation improved from 0.89976 to 0.90047, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0996 - matthews_correlation: 0.9282 - val_loss: 0.1467 - val_matthews_correlation: 0.9005\n",
      "Epoch 82/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0978 - matthews_correlation: 0.9289\n",
      "Epoch 00082: val_matthews_correlation did not improve from 0.90047\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0978 - matthews_correlation: 0.9289 - val_loss: 0.1511 - val_matthews_correlation: 0.8984\n",
      "Epoch 83/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1011 - matthews_correlation: 0.9269\n",
      "Epoch 00083: val_matthews_correlation did not improve from 0.90047\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1011 - matthews_correlation: 0.9270 - val_loss: 0.1510 - val_matthews_correlation: 0.8957\n",
      "Epoch 84/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1005 - matthews_correlation: 0.9266\n",
      "Epoch 00084: val_matthews_correlation did not improve from 0.90047\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1005 - matthews_correlation: 0.9266 - val_loss: 0.1457 - val_matthews_correlation: 0.9001\n",
      "Epoch 85/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0951 - matthews_correlation: 0.9313\n",
      "Epoch 00085: val_matthews_correlation did not improve from 0.90047\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0952 - matthews_correlation: 0.9312 - val_loss: 0.1677 - val_matthews_correlation: 0.8879\n",
      "Epoch 86/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0973 - matthews_correlation: 0.9296\n",
      "Epoch 00086: val_matthews_correlation did not improve from 0.90047\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0972 - matthews_correlation: 0.9296 - val_loss: 0.1478 - val_matthews_correlation: 0.8986\n",
      "Epoch 87/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0916 - matthews_correlation: 0.9343\n",
      "Epoch 00087: val_matthews_correlation improved from 0.90047 to 0.90390, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0915 - matthews_correlation: 0.9344 - val_loss: 0.1446 - val_matthews_correlation: 0.9039\n",
      "Epoch 88/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0915 - matthews_correlation: 0.9348\n",
      "Epoch 00088: val_matthews_correlation did not improve from 0.90390\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0916 - matthews_correlation: 0.9348 - val_loss: 0.1556 - val_matthews_correlation: 0.8952\n",
      "Epoch 89/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0931 - matthews_correlation: 0.9331\n",
      "Epoch 00089: val_matthews_correlation improved from 0.90390 to 0.90706, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0930 - matthews_correlation: 0.9332 - val_loss: 0.1406 - val_matthews_correlation: 0.9071\n",
      "Epoch 90/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0968 - matthews_correlation: 0.9302\n",
      "Epoch 00090: val_matthews_correlation did not improve from 0.90706\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0969 - matthews_correlation: 0.9301 - val_loss: 0.1484 - val_matthews_correlation: 0.8995\n",
      "Epoch 91/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0902 - matthews_correlation: 0.9354\n",
      "Epoch 00091: val_matthews_correlation improved from 0.90706 to 0.90810, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.0902 - matthews_correlation: 0.9354 - val_loss: 0.1399 - val_matthews_correlation: 0.9081\n",
      "Epoch 92/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0925 - matthews_correlation: 0.9342\n",
      "Epoch 00092: val_matthews_correlation did not improve from 0.90810\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0925 - matthews_correlation: 0.9342 - val_loss: 0.1524 - val_matthews_correlation: 0.9018\n",
      "Epoch 93/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0854 - matthews_correlation: 0.9390\n",
      "Epoch 00093: val_matthews_correlation did not improve from 0.90810\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0855 - matthews_correlation: 0.9390 - val_loss: 0.1555 - val_matthews_correlation: 0.8973\n",
      "Epoch 94/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0880 - matthews_correlation: 0.9364\n",
      "Epoch 00094: val_matthews_correlation did not improve from 0.90810\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0880 - matthews_correlation: 0.9365 - val_loss: 0.1486 - val_matthews_correlation: 0.9029\n",
      "Epoch 95/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0849 - matthews_correlation: 0.9388\n",
      "Epoch 00095: val_matthews_correlation improved from 0.90810 to 0.91086, saving model to weights_3.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0849 - matthews_correlation: 0.9388 - val_loss: 0.1413 - val_matthews_correlation: 0.9109\n",
      "Epoch 96/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0847 - matthews_correlation: 0.9389\n",
      "Epoch 00096: val_matthews_correlation did not improve from 0.91086\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.0848 - matthews_correlation: 0.9388 - val_loss: 0.1616 - val_matthews_correlation: 0.8965\n",
      "Epoch 97/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0892 - matthews_correlation: 0.9358\n",
      "Epoch 00097: val_matthews_correlation did not improve from 0.91086\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0891 - matthews_correlation: 0.9359 - val_loss: 0.1407 - val_matthews_correlation: 0.9071\n",
      "Epoch 98/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0796 - matthews_correlation: 0.9432\n",
      "Epoch 00098: val_matthews_correlation did not improve from 0.91086\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0796 - matthews_correlation: 0.9433 - val_loss: 0.1436 - val_matthews_correlation: 0.9069\n",
      "Epoch 99/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0824 - matthews_correlation: 0.9406\n",
      "Epoch 00099: val_matthews_correlation did not improve from 0.91086\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0825 - matthews_correlation: 0.9405 - val_loss: 0.1456 - val_matthews_correlation: 0.9082\n",
      "Epoch 100/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0799 - matthews_correlation: 0.9430\n",
      "Epoch 00100: val_matthews_correlation did not improve from 0.91086\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0799 - matthews_correlation: 0.9429 - val_loss: 0.1389 - val_matthews_correlation: 0.9095\n",
      "Beginning fold 5\n",
      "Train on 239389 samples, validate on 59847 samples\n",
      "Epoch 1/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.6188 - matthews_correlation: 0.3275\n",
      "Epoch 00001: val_matthews_correlation improved from -inf to 0.35146, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 29us/sample - loss: 0.6187 - matthews_correlation: 0.3276 - val_loss: 0.6077 - val_matthews_correlation: 0.3515\n",
      "Epoch 2/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.6013 - matthews_correlation: 0.3520\n",
      "Epoch 00002: val_matthews_correlation improved from 0.35146 to 0.35696, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.6013 - matthews_correlation: 0.3518 - val_loss: 0.5950 - val_matthews_correlation: 0.3570\n",
      "Epoch 3/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.5881 - matthews_correlation: 0.3661\n",
      "Epoch 00003: val_matthews_correlation improved from 0.35696 to 0.37481, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5880 - matthews_correlation: 0.3663 - val_loss: 0.5785 - val_matthews_correlation: 0.3748\n",
      "Epoch 4/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.5746 - matthews_correlation: 0.3851\n",
      "Epoch 00004: val_matthews_correlation improved from 0.37481 to 0.39011, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5746 - matthews_correlation: 0.3850 - val_loss: 0.5664 - val_matthews_correlation: 0.3901\n",
      "Epoch 5/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.5612 - matthews_correlation: 0.4002\n",
      "Epoch 00005: val_matthews_correlation improved from 0.39011 to 0.41217, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5611 - matthews_correlation: 0.4002 - val_loss: 0.5519 - val_matthews_correlation: 0.4122\n",
      "Epoch 6/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.5432 - matthews_correlation: 0.4300\n",
      "Epoch 00006: val_matthews_correlation improved from 0.41217 to 0.44243, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5432 - matthews_correlation: 0.4301 - val_loss: 0.5319 - val_matthews_correlation: 0.4424\n",
      "Epoch 7/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.5238 - matthews_correlation: 0.4619\n",
      "Epoch 00007: val_matthews_correlation improved from 0.44243 to 0.48116, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5237 - matthews_correlation: 0.4620 - val_loss: 0.5112 - val_matthews_correlation: 0.4812\n",
      "Epoch 8/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.5039 - matthews_correlation: 0.4940\n",
      "Epoch 00008: val_matthews_correlation improved from 0.48116 to 0.50175, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.5038 - matthews_correlation: 0.4943 - val_loss: 0.5008 - val_matthews_correlation: 0.5017\n",
      "Epoch 9/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.4811 - matthews_correlation: 0.5232\n",
      "Epoch 00009: val_matthews_correlation improved from 0.50175 to 0.53586, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.4811 - matthews_correlation: 0.5231 - val_loss: 0.4686 - val_matthews_correlation: 0.5359\n",
      "Epoch 10/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.4581 - matthews_correlation: 0.5575 ETA: 0s - loss: 0\n",
      "Epoch 00010: val_matthews_correlation improved from 0.53586 to 0.56907, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.4579 - matthews_correlation: 0.5578 - val_loss: 0.4520 - val_matthews_correlation: 0.5691\n",
      "Epoch 11/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.4354 - matthews_correlation: 0.5875\n",
      "Epoch 00011: val_matthews_correlation improved from 0.56907 to 0.59324, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.4353 - matthews_correlation: 0.5877 - val_loss: 0.4284 - val_matthews_correlation: 0.5932\n",
      "Epoch 12/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.4159 - matthews_correlation: 0.6128\n",
      "Epoch 00012: val_matthews_correlation improved from 0.59324 to 0.63148, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.4159 - matthews_correlation: 0.6128 - val_loss: 0.4065 - val_matthews_correlation: 0.6315\n",
      "Epoch 13/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3975 - matthews_correlation: 0.6376\n",
      "Epoch 00013: val_matthews_correlation improved from 0.63148 to 0.63932, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3974 - matthews_correlation: 0.6377 - val_loss: 0.3968 - val_matthews_correlation: 0.6393\n",
      "Epoch 14/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3785 - matthews_correlation: 0.6606\n",
      "Epoch 00014: val_matthews_correlation improved from 0.63932 to 0.65533, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3784 - matthews_correlation: 0.6605 - val_loss: 0.3804 - val_matthews_correlation: 0.6553\n",
      "Epoch 15/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.3656 - matthews_correlation: 0.6755\n",
      "Epoch 00015: val_matthews_correlation improved from 0.65533 to 0.69007, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3655 - matthews_correlation: 0.6756 - val_loss: 0.3614 - val_matthews_correlation: 0.6901\n",
      "Epoch 16/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.3474 - matthews_correlation: 0.6972\n",
      "Epoch 00016: val_matthews_correlation improved from 0.69007 to 0.69437, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3474 - matthews_correlation: 0.6972 - val_loss: 0.3513 - val_matthews_correlation: 0.6944\n",
      "Epoch 17/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3355 - matthews_correlation: 0.7101\n",
      "Epoch 00017: val_matthews_correlation improved from 0.69437 to 0.70419, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3356 - matthews_correlation: 0.7100 - val_loss: 0.3399 - val_matthews_correlation: 0.7042\n",
      "Epoch 18/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3232 - matthews_correlation: 0.7249\n",
      "Epoch 00018: val_matthews_correlation improved from 0.70419 to 0.72371, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3231 - matthews_correlation: 0.7250 - val_loss: 0.3228 - val_matthews_correlation: 0.7237\n",
      "Epoch 19/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.3115 - matthews_correlation: 0.7361\n",
      "Epoch 00019: val_matthews_correlation improved from 0.72371 to 0.73889, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3114 - matthews_correlation: 0.7363 - val_loss: 0.3123 - val_matthews_correlation: 0.7389\n",
      "Epoch 20/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.3003 - matthews_correlation: 0.7493\n",
      "Epoch 00020: val_matthews_correlation improved from 0.73889 to 0.74480, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.3002 - matthews_correlation: 0.7493 - val_loss: 0.3103 - val_matthews_correlation: 0.7448\n",
      "Epoch 21/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2934 - matthews_correlation: 0.7572\n",
      "Epoch 00021: val_matthews_correlation improved from 0.74480 to 0.74683, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2934 - matthews_correlation: 0.7572 - val_loss: 0.3082 - val_matthews_correlation: 0.7468\n",
      "Epoch 22/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2826 - matthews_correlation: 0.7670\n",
      "Epoch 00022: val_matthews_correlation improved from 0.74683 to 0.75958, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2824 - matthews_correlation: 0.7671 - val_loss: 0.2984 - val_matthews_correlation: 0.7596\n",
      "Epoch 23/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.2733 - matthews_correlation: 0.7768\n",
      "Epoch 00023: val_matthews_correlation improved from 0.75958 to 0.77822, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2733 - matthews_correlation: 0.7769 - val_loss: 0.2749 - val_matthews_correlation: 0.7782\n",
      "Epoch 24/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.2644 - matthews_correlation: 0.7854\n",
      "Epoch 00024: val_matthews_correlation improved from 0.77822 to 0.77924, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2642 - matthews_correlation: 0.7856 - val_loss: 0.2761 - val_matthews_correlation: 0.7792\n",
      "Epoch 25/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2592 - matthews_correlation: 0.7911\n",
      "Epoch 00025: val_matthews_correlation improved from 0.77924 to 0.78046, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2592 - matthews_correlation: 0.7912 - val_loss: 0.2732 - val_matthews_correlation: 0.7805\n",
      "Epoch 26/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.2491 - matthews_correlation: 0.8008\n",
      "Epoch 00026: val_matthews_correlation improved from 0.78046 to 0.78603, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2490 - matthews_correlation: 0.8009 - val_loss: 0.2679 - val_matthews_correlation: 0.7860\n",
      "Epoch 27/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2405 - matthews_correlation: 0.8099\n",
      "Epoch 00027: val_matthews_correlation improved from 0.78603 to 0.80345, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2406 - matthews_correlation: 0.8098 - val_loss: 0.2513 - val_matthews_correlation: 0.8034\n",
      "Epoch 28/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.2373 - matthews_correlation: 0.8132\n",
      "Epoch 00028: val_matthews_correlation improved from 0.80345 to 0.80415, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2371 - matthews_correlation: 0.8133 - val_loss: 0.2503 - val_matthews_correlation: 0.8042\n",
      "Epoch 29/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2300 - matthews_correlation: 0.8194\n",
      "Epoch 00029: val_matthews_correlation improved from 0.80415 to 0.80724, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2301 - matthews_correlation: 0.8192 - val_loss: 0.2441 - val_matthews_correlation: 0.8072\n",
      "Epoch 30/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2246 - matthews_correlation: 0.8243\n",
      "Epoch 00030: val_matthews_correlation did not improve from 0.80724\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2246 - matthews_correlation: 0.8242 - val_loss: 0.2511 - val_matthews_correlation: 0.8000\n",
      "Epoch 31/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.2175 - matthews_correlation: 0.8312\n",
      "Epoch 00031: val_matthews_correlation improved from 0.80724 to 0.81063, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2175 - matthews_correlation: 0.8313 - val_loss: 0.2408 - val_matthews_correlation: 0.8106\n",
      "Epoch 32/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2149 - matthews_correlation: 0.8331\n",
      "Epoch 00032: val_matthews_correlation improved from 0.81063 to 0.81593, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2150 - matthews_correlation: 0.8330 - val_loss: 0.2354 - val_matthews_correlation: 0.8159\n",
      "Epoch 33/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.2069 - matthews_correlation: 0.8394\n",
      "Epoch 00033: val_matthews_correlation improved from 0.81593 to 0.82783, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2070 - matthews_correlation: 0.8393 - val_loss: 0.2228 - val_matthews_correlation: 0.8278\n",
      "Epoch 34/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.2049 - matthews_correlation: 0.8414\n",
      "Epoch 00034: val_matthews_correlation did not improve from 0.82783\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2049 - matthews_correlation: 0.8413 - val_loss: 0.2216 - val_matthews_correlation: 0.8261\n",
      "Epoch 35/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.2008 - matthews_correlation: 0.8458\n",
      "Epoch 00035: val_matthews_correlation improved from 0.82783 to 0.83093, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.2008 - matthews_correlation: 0.8457 - val_loss: 0.2205 - val_matthews_correlation: 0.8309\n",
      "Epoch 36/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1942 - matthews_correlation: 0.8508\n",
      "Epoch 00036: val_matthews_correlation improved from 0.83093 to 0.83718, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1942 - matthews_correlation: 0.8509 - val_loss: 0.2201 - val_matthews_correlation: 0.8372\n",
      "Epoch 37/100\n",
      "239389/239389 [==============================] - ETA: 0s - loss: 0.1894 - matthews_correlation: 0.8555\n",
      "Epoch 00037: val_matthews_correlation did not improve from 0.83718\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1894 - matthews_correlation: 0.8555 - val_loss: 0.2156 - val_matthews_correlation: 0.8364\n",
      "Epoch 38/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1871 - matthews_correlation: 0.8574\n",
      "Epoch 00038: val_matthews_correlation improved from 0.83718 to 0.84877, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1870 - matthews_correlation: 0.8575 - val_loss: 0.2030 - val_matthews_correlation: 0.8488\n",
      "Epoch 39/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1851 - matthews_correlation: 0.8579\n",
      "Epoch 00039: val_matthews_correlation improved from 0.84877 to 0.85012, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1850 - matthews_correlation: 0.8579 - val_loss: 0.1991 - val_matthews_correlation: 0.8501\n",
      "Epoch 40/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1780 - matthews_correlation: 0.8651\n",
      "Epoch 00040: val_matthews_correlation did not improve from 0.85012\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1780 - matthews_correlation: 0.8651 - val_loss: 0.2106 - val_matthews_correlation: 0.8398\n",
      "Epoch 41/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1764 - matthews_correlation: 0.8665\n",
      "Epoch 00041: val_matthews_correlation improved from 0.85012 to 0.85129, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1764 - matthews_correlation: 0.8665 - val_loss: 0.1993 - val_matthews_correlation: 0.8513\n",
      "Epoch 42/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1708 - matthews_correlation: 0.8710 ETA: 0s - loss: 0.1705 - matthews_cor\n",
      "Epoch 00042: val_matthews_correlation improved from 0.85129 to 0.85252, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 27us/sample - loss: 0.1707 - matthews_correlation: 0.8711 - val_loss: 0.1989 - val_matthews_correlation: 0.8525\n",
      "Epoch 43/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1714 - matthews_correlation: 0.8712\n",
      "Epoch 00043: val_matthews_correlation did not improve from 0.85252\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1713 - matthews_correlation: 0.8713 - val_loss: 0.1976 - val_matthews_correlation: 0.8508\n",
      "Epoch 44/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1630 - matthews_correlation: 0.8793\n",
      "Epoch 00044: val_matthews_correlation improved from 0.85252 to 0.85347, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1629 - matthews_correlation: 0.8792 - val_loss: 0.1940 - val_matthews_correlation: 0.8535\n",
      "Epoch 45/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1620 - matthews_correlation: 0.8786\n",
      "Epoch 00045: val_matthews_correlation improved from 0.85347 to 0.86501, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1620 - matthews_correlation: 0.8786 - val_loss: 0.1843 - val_matthews_correlation: 0.8650\n",
      "Epoch 46/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1578 - matthews_correlation: 0.8824\n",
      "Epoch 00046: val_matthews_correlation did not improve from 0.86501\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1578 - matthews_correlation: 0.8823 - val_loss: 0.1879 - val_matthews_correlation: 0.8567\n",
      "Epoch 47/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1564 - matthews_correlation: 0.8838\n",
      "Epoch 00047: val_matthews_correlation did not improve from 0.86501\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1564 - matthews_correlation: 0.8839 - val_loss: 0.1854 - val_matthews_correlation: 0.8615\n",
      "Epoch 48/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1560 - matthews_correlation: 0.8845\n",
      "Epoch 00048: val_matthews_correlation improved from 0.86501 to 0.86585, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1560 - matthews_correlation: 0.8845 - val_loss: 0.1847 - val_matthews_correlation: 0.8659\n",
      "Epoch 49/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1505 - matthews_correlation: 0.8882\n",
      "Epoch 00049: val_matthews_correlation did not improve from 0.86585\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1506 - matthews_correlation: 0.8882 - val_loss: 0.1963 - val_matthews_correlation: 0.8541\n",
      "Epoch 50/100\n",
      "239389/239389 [==============================] - ETA: 0s - loss: 0.1475 - matthews_correlation: 0.8914\n",
      "Epoch 00050: val_matthews_correlation improved from 0.86585 to 0.86806, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1475 - matthews_correlation: 0.8914 - val_loss: 0.1824 - val_matthews_correlation: 0.8681\n",
      "Epoch 51/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1481 - matthews_correlation: 0.8904\n",
      "Epoch 00051: val_matthews_correlation improved from 0.86806 to 0.87390, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1481 - matthews_correlation: 0.8903 - val_loss: 0.1754 - val_matthews_correlation: 0.8739\n",
      "Epoch 52/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1440 - matthews_correlation: 0.8934\n",
      "Epoch 00052: val_matthews_correlation did not improve from 0.87390\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1441 - matthews_correlation: 0.8933 - val_loss: 0.1774 - val_matthews_correlation: 0.8701\n",
      "Epoch 53/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1416 - matthews_correlation: 0.8963\n",
      "Epoch 00053: val_matthews_correlation improved from 0.87390 to 0.87609, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1417 - matthews_correlation: 0.8962 - val_loss: 0.1738 - val_matthews_correlation: 0.8761\n",
      "Epoch 54/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1373 - matthews_correlation: 0.8994\n",
      "Epoch 00054: val_matthews_correlation did not improve from 0.87609\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1374 - matthews_correlation: 0.8994 - val_loss: 0.1779 - val_matthews_correlation: 0.8744\n",
      "Epoch 55/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1370 - matthews_correlation: 0.8990\n",
      "Epoch 00055: val_matthews_correlation improved from 0.87609 to 0.88405, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1370 - matthews_correlation: 0.8990 - val_loss: 0.1635 - val_matthews_correlation: 0.8841\n",
      "Epoch 56/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1349 - matthews_correlation: 0.9011\n",
      "Epoch 00056: val_matthews_correlation did not improve from 0.88405\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1349 - matthews_correlation: 0.9011 - val_loss: 0.1652 - val_matthews_correlation: 0.8802\n",
      "Epoch 57/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1308 - matthews_correlation: 0.9045\n",
      "Epoch 00057: val_matthews_correlation improved from 0.88405 to 0.88896, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1309 - matthews_correlation: 0.9044 - val_loss: 0.1564 - val_matthews_correlation: 0.8890\n",
      "Epoch 58/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1279 - matthews_correlation: 0.9070\n",
      "Epoch 00058: val_matthews_correlation did not improve from 0.88896\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1279 - matthews_correlation: 0.9069 - val_loss: 0.1656 - val_matthews_correlation: 0.8815\n",
      "Epoch 59/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1293 - matthews_correlation: 0.9069\n",
      "Epoch 00059: val_matthews_correlation did not improve from 0.88896\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1293 - matthews_correlation: 0.9068 - val_loss: 0.1600 - val_matthews_correlation: 0.8853\n",
      "Epoch 60/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1254 - matthews_correlation: 0.9091\n",
      "Epoch 00060: val_matthews_correlation did not improve from 0.88896\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1254 - matthews_correlation: 0.9090 - val_loss: 0.1573 - val_matthews_correlation: 0.8879\n",
      "Epoch 61/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1245 - matthews_correlation: 0.9101\n",
      "Epoch 00061: val_matthews_correlation improved from 0.88896 to 0.89004, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1244 - matthews_correlation: 0.9100 - val_loss: 0.1570 - val_matthews_correlation: 0.8900\n",
      "Epoch 62/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1222 - matthews_correlation: 0.9116\n",
      "Epoch 00062: val_matthews_correlation did not improve from 0.89004\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1222 - matthews_correlation: 0.9116 - val_loss: 0.1581 - val_matthews_correlation: 0.8886\n",
      "Epoch 63/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1190 - matthews_correlation: 0.9136\n",
      "Epoch 00063: val_matthews_correlation did not improve from 0.89004\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1191 - matthews_correlation: 0.9135 - val_loss: 0.1556 - val_matthews_correlation: 0.8900\n",
      "Epoch 64/100\n",
      "239389/239389 [==============================] - ETA: 0s - loss: 0.1155 - matthews_correlation: 0.9164\n",
      "Epoch 00064: val_matthews_correlation improved from 0.89004 to 0.89320, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 27us/sample - loss: 0.1155 - matthews_correlation: 0.9164 - val_loss: 0.1551 - val_matthews_correlation: 0.8932\n",
      "Epoch 65/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1154 - matthews_correlation: 0.9169\n",
      "Epoch 00065: val_matthews_correlation did not improve from 0.89320\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1154 - matthews_correlation: 0.9169 - val_loss: 0.1589 - val_matthews_correlation: 0.8887\n",
      "Epoch 66/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1166 - matthews_correlation: 0.9147\n",
      "Epoch 00066: val_matthews_correlation did not improve from 0.89320\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1166 - matthews_correlation: 0.9147 - val_loss: 0.1537 - val_matthews_correlation: 0.8925\n",
      "Epoch 67/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1143 - matthews_correlation: 0.9177\n",
      "Epoch 00067: val_matthews_correlation improved from 0.89320 to 0.89821, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1142 - matthews_correlation: 0.9178 - val_loss: 0.1489 - val_matthews_correlation: 0.8982\n",
      "Epoch 68/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1109 - matthews_correlation: 0.9210\n",
      "Epoch 00068: val_matthews_correlation improved from 0.89821 to 0.90122, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1109 - matthews_correlation: 0.9212 - val_loss: 0.1455 - val_matthews_correlation: 0.9012\n",
      "Epoch 69/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1085 - matthews_correlation: 0.9226\n",
      "Epoch 00069: val_matthews_correlation did not improve from 0.90122\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1086 - matthews_correlation: 0.9225 - val_loss: 0.1638 - val_matthews_correlation: 0.8886\n",
      "Epoch 70/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1069 - matthews_correlation: 0.9234\n",
      "Epoch 00070: val_matthews_correlation did not improve from 0.90122\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1069 - matthews_correlation: 0.9233 - val_loss: 0.1571 - val_matthews_correlation: 0.8898\n",
      "Epoch 71/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1089 - matthews_correlation: 0.9213\n",
      "Epoch 00071: val_matthews_correlation did not improve from 0.90122\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1088 - matthews_correlation: 0.9214 - val_loss: 0.1545 - val_matthews_correlation: 0.8942\n",
      "Epoch 72/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1095 - matthews_correlation: 0.9216\n",
      "Epoch 00072: val_matthews_correlation did not improve from 0.90122\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1096 - matthews_correlation: 0.9216 - val_loss: 0.1599 - val_matthews_correlation: 0.8865\n",
      "Epoch 73/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1033 - matthews_correlation: 0.9259\n",
      "Epoch 00073: val_matthews_correlation did not improve from 0.90122\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1033 - matthews_correlation: 0.9259 - val_loss: 0.1489 - val_matthews_correlation: 0.8984\n",
      "Epoch 74/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1071 - matthews_correlation: 0.9234\n",
      "Epoch 00074: val_matthews_correlation did not improve from 0.90122\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1071 - matthews_correlation: 0.9234 - val_loss: 0.1531 - val_matthews_correlation: 0.8948\n",
      "Epoch 75/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1026 - matthews_correlation: 0.9273\n",
      "Epoch 00075: val_matthews_correlation did not improve from 0.90122\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1026 - matthews_correlation: 0.9273 - val_loss: 0.1492 - val_matthews_correlation: 0.8965\n",
      "Epoch 76/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.1008 - matthews_correlation: 0.9279\n",
      "Epoch 00076: val_matthews_correlation did not improve from 0.90122\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1008 - matthews_correlation: 0.9279 - val_loss: 0.1545 - val_matthews_correlation: 0.8937\n",
      "Epoch 77/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.1015 - matthews_correlation: 0.9274\n",
      "Epoch 00077: val_matthews_correlation did not improve from 0.90122\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.1014 - matthews_correlation: 0.9274 - val_loss: 0.1622 - val_matthews_correlation: 0.8937\n",
      "Epoch 78/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0972 - matthews_correlation: 0.9307\n",
      "Epoch 00078: val_matthews_correlation did not improve from 0.90122\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0973 - matthews_correlation: 0.9307 - val_loss: 0.1514 - val_matthews_correlation: 0.8997\n",
      "Epoch 79/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0993 - matthews_correlation: 0.9291\n",
      "Epoch 00079: val_matthews_correlation improved from 0.90122 to 0.90234, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0994 - matthews_correlation: 0.9289 - val_loss: 0.1483 - val_matthews_correlation: 0.9023\n",
      "Epoch 80/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0933 - matthews_correlation: 0.9340\n",
      "Epoch 00080: val_matthews_correlation did not improve from 0.90234\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0935 - matthews_correlation: 0.9338 - val_loss: 0.1479 - val_matthews_correlation: 0.9014\n",
      "Epoch 81/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0955 - matthews_correlation: 0.9315\n",
      "Epoch 00081: val_matthews_correlation improved from 0.90234 to 0.90243, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0955 - matthews_correlation: 0.9315 - val_loss: 0.1483 - val_matthews_correlation: 0.9024\n",
      "Epoch 82/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0927 - matthews_correlation: 0.9339\n",
      "Epoch 00082: val_matthews_correlation improved from 0.90243 to 0.90796, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0928 - matthews_correlation: 0.9339 - val_loss: 0.1359 - val_matthews_correlation: 0.9080\n",
      "Epoch 83/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0881 - matthews_correlation: 0.9380\n",
      "Epoch 00083: val_matthews_correlation did not improve from 0.90796\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0881 - matthews_correlation: 0.9380 - val_loss: 0.1549 - val_matthews_correlation: 0.8983\n",
      "Epoch 84/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0949 - matthews_correlation: 0.9326\n",
      "Epoch 00084: val_matthews_correlation did not improve from 0.90796\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0950 - matthews_correlation: 0.9325 - val_loss: 0.1483 - val_matthews_correlation: 0.9043\n",
      "Epoch 85/100\n",
      "239389/239389 [==============================] - ETA: 0s - loss: 0.0941 - matthews_correlation: 0.9331\n",
      "Epoch 00085: val_matthews_correlation did not improve from 0.90796\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0941 - matthews_correlation: 0.9331 - val_loss: 0.1644 - val_matthews_correlation: 0.8922\n",
      "Epoch 86/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0935 - matthews_correlation: 0.9338\n",
      "Epoch 00086: val_matthews_correlation did not improve from 0.90796\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0935 - matthews_correlation: 0.9338 - val_loss: 0.1447 - val_matthews_correlation: 0.9054\n",
      "Epoch 87/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0851 - matthews_correlation: 0.9401\n",
      "Epoch 00087: val_matthews_correlation did not improve from 0.90796\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0852 - matthews_correlation: 0.9399 - val_loss: 0.1415 - val_matthews_correlation: 0.9058\n",
      "Epoch 88/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0861 - matthews_correlation: 0.9387\n",
      "Epoch 00088: val_matthews_correlation did not improve from 0.90796\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0861 - matthews_correlation: 0.9387 - val_loss: 0.1471 - val_matthews_correlation: 0.9041\n",
      "Epoch 89/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0844 - matthews_correlation: 0.9408\n",
      "Epoch 00089: val_matthews_correlation improved from 0.90796 to 0.90847, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0844 - matthews_correlation: 0.9409 - val_loss: 0.1422 - val_matthews_correlation: 0.9085\n",
      "Epoch 90/100\n",
      "239389/239389 [==============================] - ETA: 0s - loss: 0.0862 - matthews_correlation: 0.9388\n",
      "Epoch 00090: val_matthews_correlation improved from 0.90847 to 0.90922, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0862 - matthews_correlation: 0.9388 - val_loss: 0.1389 - val_matthews_correlation: 0.9092\n",
      "Epoch 91/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0822 - matthews_correlation: 0.9423\n",
      "Epoch 00091: val_matthews_correlation improved from 0.90922 to 0.91303, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0823 - matthews_correlation: 0.9422 - val_loss: 0.1370 - val_matthews_correlation: 0.9130\n",
      "Epoch 92/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0811 - matthews_correlation: 0.9429\n",
      "Epoch 00092: val_matthews_correlation did not improve from 0.91303\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0810 - matthews_correlation: 0.9429 - val_loss: 0.1398 - val_matthews_correlation: 0.9118\n",
      "Epoch 93/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0810 - matthews_correlation: 0.9433\n",
      "Epoch 00093: val_matthews_correlation did not improve from 0.91303\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0811 - matthews_correlation: 0.9432 - val_loss: 0.1409 - val_matthews_correlation: 0.9104\n",
      "Epoch 94/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0898 - matthews_correlation: 0.9371\n",
      "Epoch 00094: val_matthews_correlation improved from 0.91303 to 0.91563, saving model to weights_4.h5\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0897 - matthews_correlation: 0.9371 - val_loss: 0.1336 - val_matthews_correlation: 0.9156\n",
      "Epoch 95/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0808 - matthews_correlation: 0.9432\n",
      "Epoch 00095: val_matthews_correlation did not improve from 0.91563\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0809 - matthews_correlation: 0.9431 - val_loss: 0.1464 - val_matthews_correlation: 0.9032\n",
      "Epoch 96/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0797 - matthews_correlation: 0.9442\n",
      "Epoch 00096: val_matthews_correlation did not improve from 0.91563\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0795 - matthews_correlation: 0.9443 - val_loss: 0.1316 - val_matthews_correlation: 0.9149\n",
      "Epoch 97/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0812 - matthews_correlation: 0.9425\n",
      "Epoch 00097: val_matthews_correlation did not improve from 0.91563\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0812 - matthews_correlation: 0.9425 - val_loss: 0.1389 - val_matthews_correlation: 0.9143\n",
      "Epoch 98/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0806 - matthews_correlation: 0.9433\n",
      "Epoch 00098: val_matthews_correlation did not improve from 0.91563\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0806 - matthews_correlation: 0.9433 - val_loss: 0.1422 - val_matthews_correlation: 0.9125\n",
      "Epoch 99/100\n",
      "237568/239389 [============================>.] - ETA: 0s - loss: 0.0780 - matthews_correlation: 0.9449\n",
      "Epoch 00099: val_matthews_correlation did not improve from 0.91563\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0781 - matthews_correlation: 0.9450 - val_loss: 0.1374 - val_matthews_correlation: 0.9140\n",
      "Epoch 100/100\n",
      "238592/239389 [============================>.] - ETA: 0s - loss: 0.0759 - matthews_correlation: 0.9470\n",
      "Epoch 00100: val_matthews_correlation did not improve from 0.91563\n",
      "239389/239389 [==============================] - 7s 28us/sample - loss: 0.0759 - matthews_correlation: 0.9469 - val_loss: 0.1330 - val_matthews_correlation: 0.9147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((299236,), (299236,))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is where the training happens\n",
    "\n",
    "# First, create a set of indexes of the 5 folds\n",
    "splits = list(StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=2019).split(X, y_smo))\n",
    "preds_val = []\n",
    "y_val = []\n",
    "# Then, iteract with each fold\n",
    "# If you dont know, enumerate(['a', 'b', 'c']) returns [(0, 'a'), (1, 'b'), (2, 'c')]\n",
    "for idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    K.clear_session() # I dont know what it do, but I imagine that it \"clear session\" :)\n",
    "    print(\"Beginning fold {}\".format(idx+1))\n",
    "    # use the indexes to extract the folds in the train and validation data\n",
    "    train_X, train_y, val_X, val_y = X[train_idx], y_smo[train_idx], X[val_idx], y_smo[val_idx]\n",
    "    # instantiate the model for this fold\n",
    "    model = model_lstm(train_X.shape)\n",
    "    # This checkpoint helps to avoid overfitting. It just save the weights of the model if it delivered an\n",
    "    # validation matthews_correlation greater than the last one.\n",
    "    ckpt = ModelCheckpoint('weights_{}.h5'.format(idx), save_best_only=True, save_weights_only=True, verbose=1, monitor='val_matthews_correlation', mode='max')\n",
    "    # Train, train, train\n",
    "    model.fit(train_X, train_y, batch_size=1024, epochs=100, validation_data=[val_X, val_y], callbacks=[ckpt])\n",
    "    # loads the best weights saved by the checkpoint\n",
    "    model.load_weights('weights_{}.h5'.format(idx))\n",
    "    # Add the predictions of the validation to the list preds_val\n",
    "    preds_val.append(model.predict(val_X, batch_size=512))\n",
    "    # and the val true y\n",
    "    y_val.append(val_y)\n",
    "\n",
    "# concatenates all and prints the shape    \n",
    "preds_val = np.concatenate(preds_val)[...,0]\n",
    "y_val = np.concatenate(y_val)\n",
    "preds_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(val_X, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(y_pred):\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        if(y_pred[i]<0.5):\n",
    "            y_pred[i] = 0\n",
    "        else:\n",
    "            y_pred[i] = 1\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = argmax(y_pred).reshape(y_pred.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1.0: 31075, 0.0: 28772})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets,model_selection,metrics,tree,preprocessing\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test,y_pred,average='binary')\n",
    "\n",
    "recall = metrics.recall_score(y_test,y_pred, average='macro')\n",
    "\n",
    "fscore = metrics.f1_score(y_test,y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.957508312864471, 0.9405631536604988, 0.9575079914935435, 0.9574925763713762)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc,precision,recall,fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 7, 1)]            0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 7, 256)            134144    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 7, 128)            164864    \n",
      "_________________________________________________________________\n",
      "attention (Attention)        (None, 128)               135       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 307,464\n",
      "Trainable params: 307,464\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AMC]",
   "language": "python",
   "name": "conda-env-AMC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
